+ export CUDA_VISIBLE_DEVICES=4,5,6,7
+ CUDA_VISIBLE_DEVICES=4,5,6,7
+ export VLLM_ATTENTION_BACKEND=FLASH_ATTN
+ VLLM_ATTENTION_BACKEND=FLASH_ATTN
+ export VLLM_USE_FLASHINFER_SAMPLER=0
+ VLLM_USE_FLASHINFER_SAMPLER=0
+ export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False
+ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:False
+ export VLLM_USE_V1=1
+ VLLM_USE_V1=1
+ export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
+ VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
+ export VLLM_ENGINE_ITERATION_TIMEOUT_S=100000000000
+ VLLM_ENGINE_ITERATION_TIMEOUT_S=100000000000
+ export HYDRA_FULL_ERROR=1
+ HYDRA_FULL_ERROR=1
+ export CUDA_HOME=/usr/local/cuda
+ CUDA_HOME=/usr/local/cuda
+ export LD_LIBRARY_PATH=/usr/local/cuda/targets/x86_64-linux/lib:
+ LD_LIBRARY_PATH=/usr/local/cuda/targets/x86_64-linux/lib:
+ export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/targets/x86_64-linux/lib:
+ LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/targets/x86_64-linux/lib:
+ model_0_config_path=models.model_0.ppo_trainer_config
+ train_data_size=256
+ val_data_size=128
+ model_0_data_dir=/home/lah003/data/code/model_0
+ model_0_USE_GRPO='models.model_0.ppo_trainer_config.algorithm.adv_estimator=grpo models.model_0.ppo_trainer_config.actor_rollout_ref.actor.use_kl_loss=True'
+ model_0_resource='models.model_0.ppo_trainer_config.trainer.n_gpus_per_node=1 models.model_0.ppo_trainer_config.trainer.nnodes=1'
+ model_0_data='+models.model_0.ppo_trainer_config.data.train_files=/home/lah003/data/code/model_0/text/train.parquet +models.model_0.ppo_trainer_config.data.val_files=/home/lah003/data/code/model_0/text/test.parquet'
+ python3 -m pettingllms.data.preprocess.prepare --mode text --train_data_size 256 --local_dir /home/lah003/data/code/model_0 --val_data_size 256
processing data for mode: text
Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 454.42ba/s]
Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 949.37ba/s]
+ python3 -m pettingllms.trainer.train --config-path ../config/code --config-name code_eval models.model_0.ppo_trainer_config.algorithm.adv_estimator=grpo models.model_0.ppo_trainer_config.actor_rollout_ref.actor.use_kl_loss=True models.model_0.ppo_trainer_config.trainer.n_gpus_per_node=1 models.model_0.ppo_trainer_config.trainer.nnodes=1 +models.model_0.ppo_trainer_config.data.train_files=/home/lah003/data/code/model_0/text/train.parquet +models.model_0.ppo_trainer_config.data.val_files=/home/lah003/data/code/model_0/text/test.parquet
2025-08-23 22:26:12,487	INFO worker.py:1927 -- Started a local Ray instance.
[36m(train_multi_agents pid=2477709)[0m {'agent_policy_configs': {'agent_configs': {'agent_0': {'name': 'code_generator',
[36m(train_multi_agents pid=2477709)[0m                                                         'policy_name': 'code_generator_model',
[36m(train_multi_agents pid=2477709)[0m                                                         'sample_num': 4},
[36m(train_multi_agents pid=2477709)[0m                                             'agent_1': {'name': 'test_generator',
[36m(train_multi_agents pid=2477709)[0m                                                         'policy_name': 'code_generator_model',
[36m(train_multi_agents pid=2477709)[0m                                                         'sample_num': 4}},
[36m(train_multi_agents pid=2477709)[0m                           'num_agents': 2,
[36m(train_multi_agents pid=2477709)[0m                           'policy_list': ['code_generator', 'test_generator']},
[36m(train_multi_agents pid=2477709)[0m  'data': {'filter_ratio': 0.5,
[36m(train_multi_agents pid=2477709)[0m           'gen_batch_size': 32,
[36m(train_multi_agents pid=2477709)[0m           'gen_n_samples': 4,
[36m(train_multi_agents pid=2477709)[0m           'max_prompt_length': 1024,
[36m(train_multi_agents pid=2477709)[0m           'max_response_length': 8192,
[36m(train_multi_agents pid=2477709)[0m           'sample_temperature': 0.6,
[36m(train_multi_agents pid=2477709)[0m           'train_batch_size': 16,
[36m(train_multi_agents pid=2477709)[0m           'val_batch_size': 8},
[36m(train_multi_agents pid=2477709)[0m  'env': {'batched_init': True,
[36m(train_multi_agents pid=2477709)[0m          'benchmark': 'CodeForces',
[36m(train_multi_agents pid=2477709)[0m          'max_turns': 4,
[36m(train_multi_agents pid=2477709)[0m          'multi_modal': False,
[36m(train_multi_agents pid=2477709)[0m          'name': 'code_env',
[36m(train_multi_agents pid=2477709)[0m          'resolve': False},
[36m(train_multi_agents pid=2477709)[0m  'experiment_name': 'code_test',
[36m(train_multi_agents pid=2477709)[0m  'logger': ['console', 'wandb'],
[36m(train_multi_agents pid=2477709)[0m  'mode': 'validation',
[36m(train_multi_agents pid=2477709)[0m  'models': {'model_0': {'name': 'code_generator_model',
[36m(train_multi_agents pid=2477709)[0m                         'path': 'Qwen/Qwen3-4B',
[36m(train_multi_agents pid=2477709)[0m                         'ppo_trainer_config': {'actor_rollout_ref': {'actor': {'_target_': 'verl.workers.config.FSDPActorConfig',
[36m(train_multi_agents pid=2477709)[0m                                                                                'checkpoint': {'contents': ['model',
[36m(train_multi_agents pid=2477709)[0m                                                                                                            'optimizer',
[36m(train_multi_agents pid=2477709)[0m                                                                                                            'extra']},
[36m(train_multi_agents pid=2477709)[0m                                                                                'clip_ratio': 0.2,
[36m(train_multi_agents pid=2477709)[0m                                                                                'clip_ratio_c': 3.0,
[36m(train_multi_agents pid=2477709)[0m                                                                                'clip_ratio_high': 0.2,
[36m(train_multi_agents pid=2477709)[0m                                                                                'clip_ratio_low': 0.2,
[36m(train_multi_agents pid=2477709)[0m                                                                                'entropy_coeff': 0,
[36m(train_multi_agents pid=2477709)[0m                                                                                'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(train_multi_agents pid=2477709)[0m                                                                                                'fsdp_size': 8,
[36m(train_multi_agents pid=2477709)[0m                                                                                                'offload_policy': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                                'optimizer_offload': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                                'param_offload': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                                'reshard_after_forward': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                                'wrap_policy': {'min_num_params': 0}},
[36m(train_multi_agents pid=2477709)[0m                                                                                'grad_clip': 1.0,
[36m(train_multi_agents pid=2477709)[0m                                                                                'kl_loss_coef': 0.001,
[36m(train_multi_agents pid=2477709)[0m                                                                                'kl_loss_type': 'low_var_kl',
[36m(train_multi_agents pid=2477709)[0m                                                                                'loss_agg_mode': 'token-mean',
[36m(train_multi_agents pid=2477709)[0m                                                                                'optim': {'lr': 1e-06,
[36m(train_multi_agents pid=2477709)[0m                                                                                          'lr_warmup_steps': -1,
[36m(train_multi_agents pid=2477709)[0m                                                                                          'lr_warmup_steps_ratio': 0.0,
[36m(train_multi_agents pid=2477709)[0m                                                                                          'min_lr_ratio': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                          'num_cycles': 0.5,
[36m(train_multi_agents pid=2477709)[0m                                                                                          'total_training_steps': -1,
[36m(train_multi_agents pid=2477709)[0m                                                                                          'warmup_style': 'constant',
[36m(train_multi_agents pid=2477709)[0m                                                                                          'weight_decay': 0.01},
[36m(train_multi_agents pid=2477709)[0m                                                                                'ppo_epochs': 1,
[36m(train_multi_agents pid=2477709)[0m                                                                                'ppo_max_token_len_per_gpu': 8192,
[36m(train_multi_agents pid=2477709)[0m                                                                                'ppo_micro_batch_size': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                'ppo_micro_batch_size_per_gpu': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                'ppo_mini_batch_size': 512,
[36m(train_multi_agents pid=2477709)[0m                                                                                'shuffle': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                'strategy': 'fsdp',
[36m(train_multi_agents pid=2477709)[0m                                                                                'ulysses_sequence_parallel_size': 1,
[36m(train_multi_agents pid=2477709)[0m                                                                                'use_dynamic_bsz': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                'use_kl_loss': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                'use_torch_compile': True},
[36m(train_multi_agents pid=2477709)[0m                                                                      'hybrid_engine': True,
[36m(train_multi_agents pid=2477709)[0m                                                                      'model': {'enable_activation_offload': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                'enable_gradient_checkpointing': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                'external_lib': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                'lora_alpha': 16,
[36m(train_multi_agents pid=2477709)[0m                                                                                'lora_rank': 0,
[36m(train_multi_agents pid=2477709)[0m                                                                                'override_config': {},
[36m(train_multi_agents pid=2477709)[0m                                                                                'path': 'Qwen/Qwen3-4B',
[36m(train_multi_agents pid=2477709)[0m                                                                                'target_modules': 'all-linear',
[36m(train_multi_agents pid=2477709)[0m                                                                                'trust_remote_code': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                'use_fused_kernels': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                'use_liger': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                'use_remove_padding': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                'use_shm': False},
[36m(train_multi_agents pid=2477709)[0m                                                                      'ref': {'entropy_from_logits_with_chunking': False,
[36m(train_multi_agents pid=2477709)[0m                                                                              'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(train_multi_agents pid=2477709)[0m                                                                                              'param_offload': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                              'reshard_after_forward': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                              'wrap_policy': {'min_num_params': 0}},
[36m(train_multi_agents pid=2477709)[0m                                                                              'log_prob_max_token_len_per_gpu': 16384,
[36m(train_multi_agents pid=2477709)[0m                                                                              'log_prob_micro_batch_size': 1,
[36m(train_multi_agents pid=2477709)[0m                                                                              'log_prob_micro_batch_size_per_gpu': None,
[36m(train_multi_agents pid=2477709)[0m                                                                              'log_prob_use_dynamic_bsz': False,
[36m(train_multi_agents pid=2477709)[0m                                                                              'ppo_micro_batch_size': 1,
[36m(train_multi_agents pid=2477709)[0m                                                                              'ppo_micro_batch_size_per_gpu': None,
[36m(train_multi_agents pid=2477709)[0m                                                                              'strategy': 'fsdp',
[36m(train_multi_agents pid=2477709)[0m                                                                              'ulysses_sequence_parallel_size': 1,
[36m(train_multi_agents pid=2477709)[0m                                                                              'use_torch_compile': True},
[36m(train_multi_agents pid=2477709)[0m                                                                      'rollout': {'agent': {'agent_loop_config_path': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                            'custom_async_server': {'name': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                                                    'path': None},
[36m(train_multi_agents pid=2477709)[0m                                                                                            'num_workers': 512},
[36m(train_multi_agents pid=2477709)[0m                                                                                  'chat_scheduler': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'chat_template': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'disable_log_stats': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'disable_logging': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'do_sample': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'dtype': 'bfloat16',
[36m(train_multi_agents pid=2477709)[0m                                                                                  'enable_chunked_prefill': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'enforce_eager': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'engine_kwargs': {'sglang': {'attention_backend': None},
[36m(train_multi_agents pid=2477709)[0m                                                                                                    'vllm': {'swap_space': None}},
[36m(train_multi_agents pid=2477709)[0m                                                                                  'entropy_from_logits_with_chunking': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'free_cache_engine': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'gpu_memory_utilization': 0.8,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'ignore_eos': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'layered_summon': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'load_format': 'auto',
[36m(train_multi_agents pid=2477709)[0m                                                                                  'log_prob_max_token_len_per_gpu': 16384,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'log_prob_micro_batch_size': 32,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'log_prob_micro_batch_size_per_gpu': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'log_prob_use_dynamic_bsz': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'max_model_len': 8192,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'max_num_batched_tokens': 1048576,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'max_num_seqs': 512,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'mode': 'async',
[36m(train_multi_agents pid=2477709)[0m                                                                                  'multi_turn': {'enable': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                                 'format': 'chatml',
[36m(train_multi_agents pid=2477709)[0m                                                                                                 'max_turns': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                                 'tool_config_path': None},
[36m(train_multi_agents pid=2477709)[0m                                                                                  'n': 4,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'name': 'vllm',
[36m(train_multi_agents pid=2477709)[0m                                                                                  'prompt_length': 1024,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'response_length': 8192,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'temperature': 0.6,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'tensor_model_parallel_size': 4,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'top_k': -1,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'top_p': 1,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'use_fire_sampling': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'val_kwargs': {'do_sample': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                                 'n': 1,
[36m(train_multi_agents pid=2477709)[0m                                                                                                 'temperature': 0,
[36m(train_multi_agents pid=2477709)[0m                                                                                                 'top_k': -1,
[36m(train_multi_agents pid=2477709)[0m                                                                                                 'top_p': 1.0}},
[36m(train_multi_agents pid=2477709)[0m                                                                      'trainer': {'n_gpus_per_node': 4,
[36m(train_multi_agents pid=2477709)[0m                                                                                  'n_training_gpus_per_node': 4}},
[36m(train_multi_agents pid=2477709)[0m                                                'algorithm': {'adv_estimator': 'grpo',
[36m(train_multi_agents pid=2477709)[0m                                                              'clip_advantages': False,
[36m(train_multi_agents pid=2477709)[0m                                                              'gamma': 1.0,
[36m(train_multi_agents pid=2477709)[0m                                                              'kl_ctrl': {'horizon': 10000,
[36m(train_multi_agents pid=2477709)[0m                                                                          'kl_coef': 0.001,
[36m(train_multi_agents pid=2477709)[0m                                                                          'target_kl': 0.1,
[36m(train_multi_agents pid=2477709)[0m                                                                          'type': 'fixed'},
[36m(train_multi_agents pid=2477709)[0m                                                              'kl_penalty': 'kl',
[36m(train_multi_agents pid=2477709)[0m                                                              'lam': 1.0,
[36m(train_multi_agents pid=2477709)[0m                                                              'mask_truncated_samples': False,
[36m(train_multi_agents pid=2477709)[0m                                                              'norm_adv_by_std_in_grpo': True,
[36m(train_multi_agents pid=2477709)[0m                                                              'pf_ppo': {'reweight_method': 'pow',
[36m(train_multi_agents pid=2477709)[0m                                                                         'weight_pow': 2.0},
[36m(train_multi_agents pid=2477709)[0m                                                              'use_kl_in_reward': False,
[36m(train_multi_agents pid=2477709)[0m                                                              'use_pf_ppo': False},
[36m(train_multi_agents pid=2477709)[0m                                                'critic': {'_target_': 'verl.workers.config.FSDPCriticConfig',
[36m(train_multi_agents pid=2477709)[0m                                                           'enable': None,
[36m(train_multi_agents pid=2477709)[0m                                                           'forward_micro_batch_size': None,
[36m(train_multi_agents pid=2477709)[0m                                                           'forward_micro_batch_size_per_gpu': None,
[36m(train_multi_agents pid=2477709)[0m                                                           'grad_clip': 1.0,
[36m(train_multi_agents pid=2477709)[0m                                                           'model': {'_target_': 'verl.workers.config.FSDPCriticModelCfg',
[36m(train_multi_agents pid=2477709)[0m                                                                     'enable_activation_offload': False,
[36m(train_multi_agents pid=2477709)[0m                                                                     'enable_gradient_checkpointing': True,
[36m(train_multi_agents pid=2477709)[0m                                                                     'external_lib': None,
[36m(train_multi_agents pid=2477709)[0m                                                                     'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig',
[36m(train_multi_agents pid=2477709)[0m                                                                                     'forward_prefetch': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                     'fsdp_size': 8,
[36m(train_multi_agents pid=2477709)[0m                                                                                     'offload_policy': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                     'optimizer_offload': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                     'param_offload': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                     'reshard_after_forward': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                     'wrap_policy': {'min_num_params': 0}},
[36m(train_multi_agents pid=2477709)[0m                                                                     'lora_alpha': 16,
[36m(train_multi_agents pid=2477709)[0m                                                                     'lora_rank': 0,
[36m(train_multi_agents pid=2477709)[0m                                                                     'override_config': {},
[36m(train_multi_agents pid=2477709)[0m                                                                     'path': '~/models/deepseek-llm-7b-chat',
[36m(train_multi_agents pid=2477709)[0m                                                                     'target_modules': 'all-linear',
[36m(train_multi_agents pid=2477709)[0m                                                                     'tokenizer_path': '~/models/deepseek-llm-7b-chat',
[36m(train_multi_agents pid=2477709)[0m                                                                     'trust_remote_code': False,
[36m(train_multi_agents pid=2477709)[0m                                                                     'use_remove_padding': False,
[36m(train_multi_agents pid=2477709)[0m                                                                     'use_shm': False},
[36m(train_multi_agents pid=2477709)[0m                                                           'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig',
[36m(train_multi_agents pid=2477709)[0m                                                                     'lr': 1e-05,
[36m(train_multi_agents pid=2477709)[0m                                                                     'lr_warmup_steps': -1,
[36m(train_multi_agents pid=2477709)[0m                                                                     'lr_warmup_steps_ratio': 0.0,
[36m(train_multi_agents pid=2477709)[0m                                                                     'min_lr_ratio': None,
[36m(train_multi_agents pid=2477709)[0m                                                                     'total_training_steps': -1,
[36m(train_multi_agents pid=2477709)[0m                                                                     'warmup_style': 'constant',
[36m(train_multi_agents pid=2477709)[0m                                                                     'weight_decay': 0.01},
[36m(train_multi_agents pid=2477709)[0m                                                           'strategy': 'fsdp',
[36m(train_multi_agents pid=2477709)[0m                                                           'ulysses_sequence_parallel_size': 1},
[36m(train_multi_agents pid=2477709)[0m                                                'custom_reward_function': {'name': 'compute_score',
[36m(train_multi_agents pid=2477709)[0m                                                                           'path': None},
[36m(train_multi_agents pid=2477709)[0m                                                'data': {'class_name': None,
[36m(train_multi_agents pid=2477709)[0m                                                         'class_path': None,
[36m(train_multi_agents pid=2477709)[0m                                                         'custom_cls': {'name': None,
[36m(train_multi_agents pid=2477709)[0m                                                                        'path': None},
[36m(train_multi_agents pid=2477709)[0m                                                         'dataloader_num_workers': 0,
[36m(train_multi_agents pid=2477709)[0m                                                         'filter_overlong_prompts': False,
[36m(train_multi_agents pid=2477709)[0m                                                         'filter_overlong_prompts_workers': 1,
[36m(train_multi_agents pid=2477709)[0m                                                         'image_key': 'images',
[36m(train_multi_agents pid=2477709)[0m                                                         'max_prompt_length': 1024,
[36m(train_multi_agents pid=2477709)[0m                                                         'max_response_length': 8192,
[36m(train_multi_agents pid=2477709)[0m                                                         'prompt_key': 'prompt',
[36m(train_multi_agents pid=2477709)[0m                                                         'return_full_prompt': False,
[36m(train_multi_agents pid=2477709)[0m                                                         'return_raw_chat': False,
[36m(train_multi_agents pid=2477709)[0m                                                         'return_raw_input_ids': False,
[36m(train_multi_agents pid=2477709)[0m                                                         'reward_fn_key': 'data_source',
[36m(train_multi_agents pid=2477709)[0m                                                         'sampler': {'class_name': None,
[36m(train_multi_agents pid=2477709)[0m                                                                     'class_path': None},
[36m(train_multi_agents pid=2477709)[0m                                                         'shuffle': True,
[36m(train_multi_agents pid=2477709)[0m                                                         'tokenizer': None,
[36m(train_multi_agents pid=2477709)[0m                                                         'train_batch_size': 256,
[36m(train_multi_agents pid=2477709)[0m                                                         'train_files': '/home/lah003/data/code/model_0/text/train.parquet',
[36m(train_multi_agents pid=2477709)[0m                                                         'truncation': 'error',
[36m(train_multi_agents pid=2477709)[0m                                                         'trust_remote_code': False,
[36m(train_multi_agents pid=2477709)[0m                                                         'use_shm': False,
[36m(train_multi_agents pid=2477709)[0m                                                         'val_batch_size': 256,
[36m(train_multi_agents pid=2477709)[0m                                                         'val_files': '/home/lah003/data/code/model_0/text/test.parquet',
[36m(train_multi_agents pid=2477709)[0m                                                         'video_key': 'videos'},
[36m(train_multi_agents pid=2477709)[0m                                                'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig',
[36m(train_multi_agents pid=2477709)[0m                                                                    'global_tool_config': {'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig',
[36m(train_multi_agents pid=2477709)[0m                                                                                                    'controller_nsight_options': {'cuda-graph-trace': 'graph',
[36m(train_multi_agents pid=2477709)[0m                                                                                                                                  'cuda-memory-usage': 'true',
[36m(train_multi_agents pid=2477709)[0m                                                                                                                                  'trace': 'cuda,nvtx,cublas,ucx'},
[36m(train_multi_agents pid=2477709)[0m                                                                                                    'discrete': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                                    'worker_nsight_options': {'capture-range': 'cudaProfilerApi',
[36m(train_multi_agents pid=2477709)[0m                                                                                                                              'capture-range-end': None,
[36m(train_multi_agents pid=2477709)[0m                                                                                                                              'cuda-graph-trace': 'graph',
[36m(train_multi_agents pid=2477709)[0m                                                                                                                              'cuda-memory-usage': 'true',
[36m(train_multi_agents pid=2477709)[0m                                                                                                                              'kill': 'none',
[36m(train_multi_agents pid=2477709)[0m                                                                                                                              'trace': 'cuda,nvtx,cublas,ucx'}}},
[36m(train_multi_agents pid=2477709)[0m                                                                    'profile_continuous_steps': False,
[36m(train_multi_agents pid=2477709)[0m                                                                    'save_path': 'outputs/profile',
[36m(train_multi_agents pid=2477709)[0m                                                                    'steps': None,
[36m(train_multi_agents pid=2477709)[0m                                                                    'tool': None},
[36m(train_multi_agents pid=2477709)[0m                                                'ray_kwargs': {'ray_init': {'num_cpus': None},
[36m(train_multi_agents pid=2477709)[0m                                                               'timeline_json_file': None},
[36m(train_multi_agents pid=2477709)[0m                                                'reward_model': {'enable': False,
[36m(train_multi_agents pid=2477709)[0m                                                                 'forward_max_token_len_per_gpu': 32768,
[36m(train_multi_agents pid=2477709)[0m                                                                 'launch_reward_fn_async': False,
[36m(train_multi_agents pid=2477709)[0m                                                                 'max_length': None,
[36m(train_multi_agents pid=2477709)[0m                                                                 'micro_batch_size': None,
[36m(train_multi_agents pid=2477709)[0m                                                                 'micro_batch_size_per_gpu': None,
[36m(train_multi_agents pid=2477709)[0m                                                                 'model': {'external_lib': None,
[36m(train_multi_agents pid=2477709)[0m                                                                           'fsdp_config': {'fsdp_size': 8,
[36m(train_multi_agents pid=2477709)[0m                                                                                           'param_offload': False,
[36m(train_multi_agents pid=2477709)[0m                                                                                           'reshard_after_forward': True,
[36m(train_multi_agents pid=2477709)[0m                                                                                           'wrap_policy': {'min_num_params': 0}},
[36m(train_multi_agents pid=2477709)[0m                                                                           'input_tokenizer': '~/models/deepseek-llm-7b-chat',
[36m(train_multi_agents pid=2477709)[0m                                                                           'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(train_multi_agents pid=2477709)[0m                                                                           'trust_remote_code': False,
[36m(train_multi_agents pid=2477709)[0m                                                                           'use_fused_kernels': False,
[36m(train_multi_agents pid=2477709)[0m                                                                           'use_remove_padding': False,
[36m(train_multi_agents pid=2477709)[0m                                                                           'use_shm': False},
[36m(train_multi_agents pid=2477709)[0m                                                                 'reward_manager': 'naive',
[36m(train_multi_agents pid=2477709)[0m                                                                 'sandbox_fusion': {'max_concurrent': 64,
[36m(train_multi_agents pid=2477709)[0m                                                                                    'url': None},
[36m(train_multi_agents pid=2477709)[0m                                                                 'strategy': 'fsdp',
[36m(train_multi_agents pid=2477709)[0m                                                                 'ulysses_sequence_parallel_size': 1,
[36m(train_multi_agents pid=2477709)[0m                                                                 'use_dynamic_bsz': False},
[36m(train_multi_agents pid=2477709)[0m                                                'trainer': {'balance_batch': True,
[36m(train_multi_agents pid=2477709)[0m                                                            'critic_warmup': 0,
[36m(train_multi_agents pid=2477709)[0m                                                            'default_hdfs_dir': None,
[36m(train_multi_agents pid=2477709)[0m                                                            'default_local_dir': 'checkpoints/verl_examples/gsm8k',
[36m(train_multi_agents pid=2477709)[0m                                                            'del_local_ckpt_after_load': False,
[36m(train_multi_agents pid=2477709)[0m                                                            'device': 'cuda',
[36m(train_multi_agents pid=2477709)[0m                                                            'experiment_name': 'gsm8k',
[36m(train_multi_agents pid=2477709)[0m                                                            'log_val_generations': 0,
[36m(train_multi_agents pid=2477709)[0m                                                            'logger': ['console',
[36m(train_multi_agents pid=2477709)[0m                                                                       'wandb'],
[36m(train_multi_agents pid=2477709)[0m                                                            'max_actor_ckpt_to_keep': None,
[36m(train_multi_agents pid=2477709)[0m                                                            'max_critic_ckpt_to_keep': None,
[36m(train_multi_agents pid=2477709)[0m                                                            'n_gpus_per_node': 1,
[36m(train_multi_agents pid=2477709)[0m                                                            'n_training_gpus_per_node': 4,
[36m(train_multi_agents pid=2477709)[0m                                                            'nnodes': 1,
[36m(train_multi_agents pid=2477709)[0m                                                            'npu_profile': {'options': {}},
[36m(train_multi_agents pid=2477709)[0m                                                            'project_name': 'verl_examples',
[36m(train_multi_agents pid=2477709)[0m                                                            'ray_wait_register_center_timeout': 300,
[36m(train_multi_agents pid=2477709)[0m                                                            'rejection_sample': False,
[36m(train_multi_agents pid=2477709)[0m                                                            'rejection_sample_multiplier': 2,
[36m(train_multi_agents pid=2477709)[0m                                                            'resume_from_path': None,
[36m(train_multi_agents pid=2477709)[0m                                                            'resume_mode': 'auto',
[36m(train_multi_agents pid=2477709)[0m                                                            'rollout_data_dir': None,
[36m(train_multi_agents pid=2477709)[0m                                                            'save_freq': -1,
[36m(train_multi_agents pid=2477709)[0m                                                            'test_freq': -1,
[36m(train_multi_agents pid=2477709)[0m                                                            'total_epochs': 30,
[36m(train_multi_agents pid=2477709)[0m                                                            'total_training_steps': None,
[36m(train_multi_agents pid=2477709)[0m                                                            'val_before_train': True,
[36m(train_multi_agents pid=2477709)[0m                                                            'validation_data_dir': None}}}},
[36m(train_multi_agents pid=2477709)[0m  'multi_agent_interaction': {'num_interacting_agents': 2,
[36m(train_multi_agents pid=2477709)[0m                              'shared_observation': True,
[36m(train_multi_agents pid=2477709)[0m                              'turn_order': ['code_generator',
[36m(train_multi_agents pid=2477709)[0m                                             'test_generator']},
[36m(train_multi_agents pid=2477709)[0m  'project_name': 'pettingllms',
[36m(train_multi_agents pid=2477709)[0m  'resource': {'n_gpus_per_node': 4, 'nnodes': 1, 'trust_remote_code': True},
[36m(train_multi_agents pid=2477709)[0m  'sample_mode': 'env',
[36m(train_multi_agents pid=2477709)[0m  'trainer': {'balance_batch': True,
[36m(train_multi_agents pid=2477709)[0m              'critic_warmup': 0,
[36m(train_multi_agents pid=2477709)[0m              'default_hdfs_dir': None,
[36m(train_multi_agents pid=2477709)[0m              'default_local_dir': 'checkpoints/pettingllms/code_eval',
[36m(train_multi_agents pid=2477709)[0m              'del_local_ckpt_after_load': False,
[36m(train_multi_agents pid=2477709)[0m              'device': 'cuda',
[36m(train_multi_agents pid=2477709)[0m              'experiment_name': 'code_eval',
[36m(train_multi_agents pid=2477709)[0m              'log_val_generations': 0,
[36m(train_multi_agents pid=2477709)[0m              'logger': ['console', 'wandb'],
[36m(train_multi_agents pid=2477709)[0m              'max_actor_ckpt_to_keep': None,
[36m(train_multi_agents pid=2477709)[0m              'max_critic_ckpt_to_keep': None,
[36m(train_multi_agents pid=2477709)[0m              'n_gpus_per_node': 4,
[36m(train_multi_agents pid=2477709)[0m              'n_training_gpus_per_node': 4,
[36m(train_multi_agents pid=2477709)[0m              'nnodes': 1,
[36m(train_multi_agents pid=2477709)[0m              'npu_profile': {'options': {}},
[36m(train_multi_agents pid=2477709)[0m              'project_name': 'pettingllms',
[36m(train_multi_agents pid=2477709)[0m              'ray_wait_register_center_timeout': 300,
[36m(train_multi_agents pid=2477709)[0m              'rejection_sample': False,
[36m(train_multi_agents pid=2477709)[0m              'rejection_sample_multiplier': 2,
[36m(train_multi_agents pid=2477709)[0m              'resume_from_path': None,
[36m(train_multi_agents pid=2477709)[0m              'resume_mode': 'auto',
[36m(train_multi_agents pid=2477709)[0m              'rollout_data_dir': None,
[36m(train_multi_agents pid=2477709)[0m              'save_freq': -1,
[36m(train_multi_agents pid=2477709)[0m              'test_freq': -1,
[36m(train_multi_agents pid=2477709)[0m              'total_epochs': 30,
[36m(train_multi_agents pid=2477709)[0m              'total_training_steps': None,
[36m(train_multi_agents pid=2477709)[0m              'val_before_train': True,
[36m(train_multi_agents pid=2477709)[0m              'validation_data_dir': None}}
[36m(train_multi_agents pid=2477709)[0m Multi-model training mode detected
[36m(train_multi_agents pid=2477709)[0m Processing model: code_generator_model at path: Qwen/Qwen3-4B
[36m(train_multi_agents pid=2477709)[0m [92mAgent mapping: code_generator -> code_generator_model[0m
[36m(train_multi_agents pid=2477709)[0m [92mAgent mapping: test_generator -> code_generator_model[0m
[36m(train_multi_agents pid=2477709)[0m model_config: {'ppo_trainer_config': {'data': {'tokenizer': None, 'use_shm': False, 'train_batch_size': 256, 'val_batch_size': 256, 'prompt_key': 'prompt', 'reward_fn_key': 'data_source', 'max_prompt_length': 1024, 'max_response_length': 8192, 'dataloader_num_workers': 0, 'return_raw_input_ids': False, 'return_raw_chat': False, 'return_full_prompt': False, 'shuffle': True, 'filter_overlong_prompts': False, 'filter_overlong_prompts_workers': 1, 'truncation': 'error', 'image_key': 'images', 'video_key': 'videos', 'trust_remote_code': False, 'custom_cls': {'path': None, 'name': None}, 'sampler': {'class_path': None, 'class_name': None}, 'class_path': None, 'class_name': None, 'train_files': '/home/lah003/data/code/model_0/text/train.parquet', 'val_files': '/home/lah003/data/code/model_0/text/test.parquet'}, 'actor_rollout_ref': {'hybrid_engine': True, 'model': {'path': 'Qwen/Qwen3-4B', 'use_shm': False, 'external_lib': None, 'override_config': {}, 'enable_gradient_checkpointing': True, 'enable_activation_offload': False, 'use_remove_padding': False, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear', 'use_liger': False, 'use_fused_kernels': False, 'trust_remote_code': False}, 'actor': {'_target_': 'verl.workers.config.FSDPActorConfig', 'strategy': 'fsdp', 'ppo_mini_batch_size': 512, 'use_dynamic_bsz': True, 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': None, 'ppo_max_token_len_per_gpu': 8192, 'grad_clip': 1.0, 'clip_ratio': 0.2, 'clip_ratio_low': 0.2, 'clip_ratio_high': 0.2, 'clip_ratio_c': 3.0, 'loss_agg_mode': 'token-mean', 'entropy_coeff': 0, 'use_kl_loss': True, 'use_torch_compile': True, 'kl_loss_coef': 0.001, 'kl_loss_type': 'low_var_kl', 'ppo_epochs': 1, 'shuffle': False, 'ulysses_sequence_parallel_size': 1, 'checkpoint': {'contents': ['model', 'optimizer', 'extra']}, 'optim': {'lr': 1e-06, 'lr_warmup_steps': -1, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'num_cycles': 0.5, 'warmup_style': 'constant', 'total_training_steps': -1, 'weight_decay': 0.01}, 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'optimizer_offload': False, 'offload_policy': False, 'reshard_after_forward': True, 'fsdp_size': 8}}, 'ref': {'strategy': 'fsdp', 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'param_offload': False, 'reshard_after_forward': True, 'wrap_policy': {'min_num_params': 0}}, 'use_torch_compile': True, 'log_prob_micro_batch_size': 1, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': False, 'ppo_micro_batch_size': 1, 'ppo_micro_batch_size_per_gpu': None, 'log_prob_max_token_len_per_gpu': 16384, 'ulysses_sequence_parallel_size': 1, 'entropy_from_logits_with_chunking': False}, 'rollout': {'name': 'vllm', 'mode': 'async', 'chat_scheduler': None, 'chat_template': None, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'use_fire_sampling': False, 'prompt_length': 1024, 'response_length': 8192, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.8, 'ignore_eos': False, 'enforce_eager': True, 'free_cache_engine': False, 'load_format': 'auto', 'layered_summon': False, 'tensor_model_parallel_size': 4, 'max_num_batched_tokens': 1048576, 'max_model_len': 8192, 'max_num_seqs': 512, 'log_prob_micro_batch_size': 32, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': False, 'log_prob_max_token_len_per_gpu': 16384, 'disable_log_stats': True, 'enable_chunked_prefill': True, 'do_sample': True, 'n': 4, 'engine_kwargs': {'vllm': {'swap_space': None}, 'sglang': {'attention_backend': None}}, 'val_kwargs': {'top_k': -1, 'top_p': 1.0, 'temperature': 0, 'n': 1, 'do_sample': False}, 'multi_turn': {'enable': False, 'max_turns': None, 'tool_config_path': None, 'format': 'chatml'}, 'disable_logging': True, 'entropy_from_logits_with_chunking': False, 'agent': {'num_workers': 512, 'agent_loop_config_path': None, 'custom_async_server': {'path': None, 'name': None}}}, 'trainer': {'n_gpus_per_node': 4, 'n_training_gpus_per_node': 4}}, 'critic': {'_target_': 'verl.workers.config.FSDPCriticConfig', 'enable': None, 'strategy': 'fsdp', 'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig', 'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'total_training_steps': -1, 'weight_decay': 0.01, 'lr_warmup_steps': -1, 'min_lr_ratio': None, 'warmup_style': 'constant'}, 'model': {'_target_': 'verl.workers.config.FSDPCriticModelCfg', 'path': '~/models/deepseek-llm-7b-chat', 'tokenizer_path': '~/models/deepseek-llm-7b-chat', 'override_config': {}, 'external_lib': None, 'trust_remote_code': False, 'use_shm': False, 'enable_gradient_checkpointing': True, 'enable_activation_offload': False, 'use_remove_padding': False, 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'param_offload': False, 'optimizer_offload': False, 'offload_policy': False, 'reshard_after_forward': True, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': 8, 'forward_prefetch': False}, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear'}, 'forward_micro_batch_size': None, 'forward_micro_batch_size_per_gpu': None, 'ulysses_sequence_parallel_size': 1, 'grad_clip': 1.0}, 'reward_model': {'enable': False, 'strategy': 'fsdp', 'model': {'input_tokenizer': '~/models/deepseek-llm-7b-chat', 'path': '~/models/FsfairX-LLaMA3-RM-v0.1', 'use_shm': False, 'external_lib': None, 'use_remove_padding': False, 'use_fused_kernels': False, 'trust_remote_code': False, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'reshard_after_forward': True, 'fsdp_size': 8}}, 'micro_batch_size': None, 'micro_batch_size_per_gpu': None, 'max_length': None, 'ulysses_sequence_parallel_size': 1, 'use_dynamic_bsz': False, 'forward_max_token_len_per_gpu': 32768, 'reward_manager': 'naive', 'launch_reward_fn_async': False, 'sandbox_fusion': {'url': None, 'max_concurrent': 64}}, 'custom_reward_function': {'path': None, 'name': 'compute_score'}, 'algorithm': {'adv_estimator': 'grpo', 'gamma': 1.0, 'lam': 1.0, 'norm_adv_by_std_in_grpo': True, 'use_kl_in_reward': False, 'kl_penalty': 'kl', 'kl_ctrl': {'type': 'fixed', 'kl_coef': 0.001, 'horizon': 10000, 'target_kl': 0.1}, 'use_pf_ppo': False, 'pf_ppo': {'reweight_method': 'pow', 'weight_pow': 2.0}, 'mask_truncated_samples': False, 'clip_advantages': False}, 'trainer': {'device': 'cuda', 'n_gpus_per_node': 1, 'nnodes': 1, 'balance_batch': True, 'total_epochs': 30, 'total_training_steps': None, 'project_name': 'verl_examples', 'experiment_name': 'gsm8k', 'logger': ['console', 'wandb'], 'log_val_generations': 0, 'rollout_data_dir': None, 'validation_data_dir': None, 'save_freq': -1, 'resume_mode': 'auto', 'resume_from_path': None, 'val_before_train': True, 'test_freq': -1, 'critic_warmup': 0, 'default_hdfs_dir': None, 'del_local_ckpt_after_load': False, 'default_local_dir': 'checkpoints/verl_examples/gsm8k', 'max_actor_ckpt_to_keep': None, 'max_critic_ckpt_to_keep': None, 'ray_wait_register_center_timeout': 300, 'npu_profile': {'options': {}}, 'rejection_sample': False, 'rejection_sample_multiplier': 2, 'n_training_gpus_per_node': 4}, 'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig', 'tool': None, 'steps': None, 'profile_continuous_steps': False, 'save_path': 'outputs/profile', 'global_tool_config': {'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig', 'discrete': False, 'controller_nsight_options': {'trace': 'cuda,nvtx,cublas,ucx', 'cuda-memory-usage': 'true', 'cuda-graph-trace': 'graph'}, 'worker_nsight_options': {'trace': 'cuda,nvtx,cublas,ucx', 'cuda-memory-usage': 'true', 'cuda-graph-trace': 'graph', 'capture-range': 'cudaProfilerApi', 'capture-range-end': None, 'kill': 'none'}}}}, 'ray_kwargs': {'ray_init': {'num_cpus': None}, 'timeline_json_file': None}}, 'path': 'Qwen/Qwen3-4B', 'name': 'code_generator_model'}
[36m(train_multi_agents pid=2477709)[0m ppo_config: {'data': {'tokenizer': None, 'use_shm': False, 'train_batch_size': 16, 'val_batch_size': 8, 'prompt_key': 'prompt', 'reward_fn_key': 'data_source', 'max_prompt_length': 1024, 'max_response_length': 8192, 'dataloader_num_workers': 0, 'return_raw_input_ids': False, 'return_raw_chat': False, 'return_full_prompt': False, 'shuffle': True, 'filter_overlong_prompts': False, 'filter_overlong_prompts_workers': 1, 'truncation': 'error', 'image_key': 'images', 'video_key': 'videos', 'trust_remote_code': False, 'custom_cls': {'path': None, 'name': None}, 'sampler': {'class_path': None, 'class_name': None}, 'class_path': None, 'class_name': None, 'train_files': '/home/lah003/data/code/model_0/text/train.parquet', 'val_files': '/home/lah003/data/code/model_0/text/test.parquet'}, 'actor_rollout_ref': {'hybrid_engine': True, 'model': {'path': 'Qwen/Qwen3-4B', 'use_shm': False, 'external_lib': None, 'override_config': {}, 'enable_gradient_checkpointing': True, 'enable_activation_offload': False, 'use_remove_padding': False, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear', 'use_liger': False, 'use_fused_kernels': False, 'trust_remote_code': False}, 'actor': {'_target_': 'verl.workers.config.FSDPActorConfig', 'strategy': 'fsdp', 'ppo_mini_batch_size': 512, 'use_dynamic_bsz': True, 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': None, 'ppo_max_token_len_per_gpu': 8192, 'grad_clip': 1.0, 'clip_ratio': 0.2, 'clip_ratio_low': 0.2, 'clip_ratio_high': 0.2, 'clip_ratio_c': 3.0, 'loss_agg_mode': 'token-mean', 'entropy_coeff': 0, 'use_kl_loss': True, 'use_torch_compile': True, 'kl_loss_coef': 0.001, 'kl_loss_type': 'low_var_kl', 'ppo_epochs': 1, 'shuffle': False, 'ulysses_sequence_parallel_size': 1, 'checkpoint': {'contents': ['model', 'optimizer', 'extra']}, 'optim': {'lr': 1e-06, 'lr_warmup_steps': -1, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'num_cycles': 0.5, 'warmup_style': 'constant', 'total_training_steps': -1, 'weight_decay': 0.01}, 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'optimizer_offload': False, 'offload_policy': False, 'reshard_after_forward': True, 'fsdp_size': 8}}, 'ref': {'strategy': 'fsdp', 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'param_offload': False, 'reshard_after_forward': True, 'wrap_policy': {'min_num_params': 0}}, 'use_torch_compile': True, 'log_prob_micro_batch_size': 1, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': False, 'ppo_micro_batch_size': 1, 'ppo_micro_batch_size_per_gpu': None, 'log_prob_max_token_len_per_gpu': 16384, 'ulysses_sequence_parallel_size': 1, 'entropy_from_logits_with_chunking': False}, 'rollout': {'name': 'vllm', 'mode': 'async', 'chat_scheduler': None, 'chat_template': None, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'use_fire_sampling': False, 'prompt_length': 1024, 'response_length': 8192, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.8, 'ignore_eos': False, 'enforce_eager': True, 'free_cache_engine': False, 'load_format': 'auto', 'layered_summon': False, 'tensor_model_parallel_size': 4, 'max_num_batched_tokens': 1048576, 'max_model_len': 8192, 'max_num_seqs': 512, 'log_prob_micro_batch_size': 32, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': False, 'log_prob_max_token_len_per_gpu': 16384, 'disable_log_stats': True, 'enable_chunked_prefill': True, 'do_sample': True, 'n': 4, 'engine_kwargs': {'vllm': {'swap_space': None}, 'sglang': {'attention_backend': None}}, 'val_kwargs': {'top_k': -1, 'top_p': 1.0, 'temperature': 0, 'n': 1, 'do_sample': False}, 'multi_turn': {'enable': False, 'max_turns': None, 'tool_config_path': None, 'format': 'chatml'}, 'disable_logging': True, 'entropy_from_logits_with_chunking': False, 'agent': {'num_workers': 512, 'agent_loop_config_path': None, 'custom_async_server': {'path': None, 'name': None}}}, 'trainer': {'n_gpus_per_node': 4, 'n_training_gpus_per_node': 4}}, 'critic': {'_target_': 'verl.workers.config.FSDPCriticConfig', 'enable': None, 'strategy': 'fsdp', 'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig', 'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'total_training_steps': -1, 'weight_decay': 0.01, 'lr_warmup_steps': -1, 'min_lr_ratio': None, 'warmup_style': 'constant'}, 'model': {'_target_': 'verl.workers.config.FSDPCriticModelCfg', 'path': '~/models/deepseek-llm-7b-chat', 'tokenizer_path': '~/models/deepseek-llm-7b-chat', 'override_config': {}, 'external_lib': None, 'trust_remote_code': False, 'use_shm': False, 'enable_gradient_checkpointing': True, 'enable_activation_offload': False, 'use_remove_padding': False, 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'param_offload': False, 'optimizer_offload': False, 'offload_policy': False, 'reshard_after_forward': True, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': 8, 'forward_prefetch': False}, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear'}, 'forward_micro_batch_size': None, 'forward_micro_batch_size_per_gpu': None, 'ulysses_sequence_parallel_size': 1, 'grad_clip': 1.0}, 'reward_model': {'enable': False, 'strategy': 'fsdp', 'model': {'input_tokenizer': '~/models/deepseek-llm-7b-chat', 'path': '~/models/FsfairX-LLaMA3-RM-v0.1', 'use_shm': False, 'external_lib': None, 'use_remove_padding': False, 'use_fused_kernels': False, 'trust_remote_code': False, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'reshard_after_forward': True, 'fsdp_size': 8}}, 'micro_batch_size': None, 'micro_batch_size_per_gpu': None, 'max_length': None, 'ulysses_sequence_parallel_size': 1, 'use_dynamic_bsz': False, 'forward_max_token_len_per_gpu': 32768, 'reward_manager': 'naive', 'launch_reward_fn_async': False, 'sandbox_fusion': {'url': None, 'max_concurrent': 64}}, 'custom_reward_function': {'path': None, 'name': 'compute_score'}, 'algorithm': {'adv_estimator': 'grpo', 'gamma': 1.0, 'lam': 1.0, 'norm_adv_by_std_in_grpo': True, 'use_kl_in_reward': False, 'kl_penalty': 'kl', 'kl_ctrl': {'type': 'fixed', 'kl_coef': 0.001, 'horizon': 10000, 'target_kl': 0.1}, 'use_pf_ppo': False, 'pf_ppo': {'reweight_method': 'pow', 'weight_pow': 2.0}, 'mask_truncated_samples': False, 'clip_advantages': False}, 'trainer': {'device': 'cuda', 'n_gpus_per_node': 1, 'nnodes': 1, 'balance_batch': True, 'total_epochs': 30, 'total_training_steps': None, 'project_name': 'verl_examples', 'experiment_name': 'gsm8k', 'logger': ['console', 'wandb'], 'log_val_generations': 0, 'rollout_data_dir': None, 'validation_data_dir': None, 'save_freq': -1, 'resume_mode': 'auto', 'resume_from_path': None, 'val_before_train': True, 'test_freq': -1, 'critic_warmup': 0, 'default_hdfs_dir': None, 'del_local_ckpt_after_load': False, 'default_local_dir': 'checkpoints/verl_examples/gsm8k', 'max_actor_ckpt_to_keep': None, 'max_critic_ckpt_to_keep': None, 'ray_wait_register_center_timeout': 300, 'npu_profile': {'options': {}}, 'rejection_sample': False, 'rejection_sample_multiplier': 2, 'n_training_gpus_per_node': 4}, 'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig', 'tool': None, 'steps': None, 'profile_continuous_steps': False, 'save_path': 'outputs/profile', 'global_tool_config': {'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig', 'discrete': False, 'controller_nsight_options': {'trace': 'cuda,nvtx,cublas,ucx', 'cuda-memory-usage': 'true', 'cuda-graph-trace': 'graph'}, 'worker_nsight_options': {'trace': 'cuda,nvtx,cublas,ucx', 'cuda-memory-usage': 'true', 'cuda-graph-trace': 'graph', 'capture-range': 'cudaProfilerApi', 'capture-range-end': None, 'kill': 'none'}}}}, 'ray_kwargs': {'ray_init': {'num_cpus': None}, 'timeline_json_file': None}}
[36m(train_multi_agents pid=2477709)[0m /home/lah003/workspace/PettingLLMs/pettingllms/trainer/multi_agents_ppo_trainer.py:107: UserWarning: Disabled critic as algorithm.adv_estimator != gae. If it is not intended, please set critic.enable=True
[36m(train_multi_agents pid=2477709)[0m   ppo_trainer = RayPPOTrainer(
[36m(train_multi_agents pid=2477709)[0m Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 256 examples [00:00, 36628.98 examples/s]
[36m(train_multi_agents pid=2477709)[0m ppo_config (partial): {'data': {'tokenizer': None, 'use_shm': False, 'train_batch_size': 16, 'val_batch_size': 8, 'prompt_key': 'prompt', 'reward_fn_key': 'data_source', 'max_prompt_length': 1024, 'max_response_length': 8192, 'dataloader_num_workers': 0, 'return_raw_input_ids': False, 'return_raw_chat': False, 'return_full_prompt': False, 'shuffle': True, 'filter_overlong_prompts': False, 'filter_overlong_prompts_workers': 1, 'truncation': 'error', 'image_key': 'images', 'video_key': 'videos', 'trust_remote_code': False, 'custom_cls': {'path': None, 'name': None}, 'sampler': {'class_path': None, 'class_name': None}, 'class_path': None, 'class_name': None, 'train_files': '/home/lah003/data/code/model_0/text/train.parquet', 'val_files': '/home/lah003/data/code/model_0/text/test.parquet'}, 'actor_rollout_ref': {'hybrid_engine': True, 'model': {'path': 'Qwen/Qwen3-4B', 'use_shm': False, 'external_lib': None, 'override_config': {}, 'enable_gradient_checkpointing': True, 'enable_activation_offload': False, 'use_remove_padding': False, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear', 'use_liger': False, 'use_fused_kernels': False, 'trust_remote_code': False}, 'actor': {'_target_': 'verl.workers.config.FSDPActorConfig', 'strategy': 'fsdp', 'ppo_mini_batch_size': 512, 'use_dynamic_bsz': True, 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': None, 'ppo_max_token_len_per_gpu': 8192, 'grad_clip': 1.0, 'clip_ratio': 0.2, 'clip_ratio_low': 0.2, 'clip_ratio_high': 0.2, 'clip_ratio_c': 3.0, 'loss_agg_mode': 'token-mean', 'entropy_coeff': 0, 'use_kl_loss': True, 'use_torch_compile': True, 'kl_loss_coef': 0.001, 'kl_loss_type': 'low_var_kl', 'ppo_epochs': 1, 'shuffle': False, 'ulysses_sequence_parallel_size': 1, 'checkpoint': {'contents': ['model', 'optimizer', 'extra']}, 'optim': {'lr': 1e-06, 'lr_warmup_steps': -1, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'num_cycles': 0.5, 'warmup_style': 'constant', 'total_training_steps': -1, 'weight_decay': 0.01}, 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'optimizer_offload': False, 'offload_policy': False, 'reshard_after_forward': True, 'fsdp_size': 8}}, 'ref': {'strategy': 'fsdp', 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'param_offload': False, 'reshard_after_forward': True, 'wrap_policy': {'min_num_params': 0}}, 'use_torch_compile': True, 'log_prob_micro_batch_size': 1, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': False, 'ppo_micro_batch_size': 1, 'ppo_micro_batch_size_per_gpu': None, 'log_prob_max_token_len_per_gpu': 16384, 'ulysses_sequence_parallel_size': 1, 'entropy_from_logits_with_chunking': False}, 'rollout': {'name': 'vllm', 'mode': 'async', 'chat_scheduler': None, 'chat_template': None, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'use_fire_sampling': False, 'prompt_length': 1024, 'response_length': 8192, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.8, 'ignore_eos': False, 'enforce_eager': True, 'free_cache_engine': False, 'load_format': 'auto', 'layered_summon': False, 'tensor_model_parallel_size': 4, 'max_num_batched_tokens': 1048576, 'max_model_len': 8192, 'max_num_seqs': 512, 'log_prob_micro_batch_size': 32, 'log_prob_micro_batch_size_per_gpu': None, 'log_prob_use_dynamic_bsz': False, 'log_prob_max_token_len_per_gpu': 16384, 'disable_log_stats': True, 'enable_chunked_prefill': True, 'do_sample': True, 'n': 4, 'engine_kwargs': {'vllm': {'swap_space': None}, 'sglang': {'attention_backend': None}}, 'val_kwargs': {'top_k': -1, 'top_p': 1.0, 'temperature': 0, 'n': 1, 'do_sample': False}, 'multi_turn': {'enable': False, 'max_turns': None, 'tool_config_path': None, 'format': 'chatml'}, 'disable_logging': True, 'entropy_from_logits_with_chunking': False, 'agent': {'num_workers': 512, 'agent_loop_config_path': None, 'custom_async_server': {'path': None, 'name': None}}}, 'trainer': {'n_gpus_per_node': 4, 'n_training_gpus_per_node': 4}}, 'critic': {'_target_': 'verl.workers.config.FSDPCriticConfig', 'enable': None, 'strategy': 'fsdp', 'optim': {'_target_': 'verl.workers.config.FSDPOptimizerConfig', 'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'total_training_steps': -1, 'weight_decay': 0.01, 'lr_warmup_steps': -1, 'min_lr_ratio': None, 'warmup_style': 'constant'}, 'model': {'_target_': 'verl.workers.config.FSDPCriticModelCfg', 'path': '~/models/deepseek-llm-7b-chat', 'tokenizer_path': '~/models/deepseek-llm-7b-chat', 'override_config': {}, 'external_lib': None, 'trust_remote_code': False, 'use_shm': False, 'enable_gradient_checkpointing': True, 'enable_activation_offload': False, 'use_remove_padding': False, 'fsdp_config': {'_target_': 'verl.workers.config.FSDPEngineConfig', 'param_offload': False, 'optimizer_offload': False, 'offload_policy': False, 'reshard_after_forward': True, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': 8, 'forward_prefetch': False}, 'lora_rank': 0, 'lora_alpha': 16, 'target_modules': 'all-linear'}, 'forward_micro_batch_size': None, 'forward_micro_batch_size_per_gpu': None, 'ulysses_sequence_parallel_size': 1, 'grad_clip': 1.0}, 'reward_model': {'enable': False, 'strategy': 'fsdp', 'model': {'input_tokenizer': '~/models/deepseek-llm-7b-chat', 'path': '~/models/FsfairX-LLaMA3-RM-v0.1', 'use_shm': False, 'external_lib': None, 'use_remove_padding': False, 'use_fused_kernels': False, 'trust_remote_code': False, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'reshard_after_forward': True, 'fsdp_size': 8}}, 'micro_batch_size': None, 'micro_batch_size_per_gpu': None, 'max_length': None, 'ulysses_sequence_parallel_size': 1, 'use_dynamic_bsz': False, 'forward_max_token_len_per_gpu': 32768, 'reward_manager': 'naive', 'launch_reward_fn_async': False, 'sandbox_fusion': {'url': None, 'max_concurrent': 64}}, 'custom_reward_function': {'path': None, 'name': 'compute_score'}, 'algorithm': {'adv_estimator': 'grpo', 'gamma': 1.0, 'lam': 1.0, 'norm_adv_by_std_in_grpo': True, 'use_kl_in_reward': False, 'kl_penalty': 'kl', 'kl_ctrl': {'type': 'fixed', 'kl_coef': 0.001, 'horizon': 10000, 'target_kl': 0.1}, 'use_pf_ppo': False, 'pf_ppo': {'reweight_method': 'pow', 'weight_pow': 2.0}, 'mask_truncated_samples': False, 'clip_advantages': False}, 'trainer': {'device': 'cuda', 'n_gpus_per_node': 1, 'nnodes': 1, 'balance_batch': True, 'total_epochs': 30, 'total_training_steps': None, 'project_name': 'verl_examples', 'experiment_name': 'gsm8k', 'logger': ['console', 'wandb'], 'log_val_generations': 0, 'rollout_data_dir': None, 'validation_data_dir': None, 'save_freq': -1, 'resume_mode': 'auto', 'resume_from_path': None, 'val_before_train': True, 'test_freq': -1, 'critic_warmup': 0, 'default_hdfs_dir': None, 'del_local_ckpt_after_load': False, 'default_local_dir': 'checkpoints/verl_examples/gsm8k', 'max_actor_ckpt_to_keep': None, 'max_critic_ckpt_to_keep': None, 'ray_wait_register_center_timeout': 300, 'npu_profile': {'options': {}}, 'rejection_sample': False, 'rejection_sample_multiplier': 2, 'n_training_gpus_per_node': 4}, 'global_profiler': {'_target_': 'verl.utils.profiler.ProfilerConfig', 'tool': None, 'steps': None, 'profile_continuous_steps': False, 'save_path': 'outputs/profile', 'global_tool_config': {'nsys': {'_target_': 'verl.utils.profiler.config.NsightToolConfig', 'discrete': False, 'controller_nsight_options': {'trace': 'cuda,nvtx,cublas,ucx', 'cuda-memory-usage': 'true', 'cuda-graph-trace': 'graph'}, 'worker_nsight_options': {'trace': 'cuda,nvtx,cublas,ucx', 'cuda-memory-usage': 'true', 'cuda-graph-trace': 'graph', 'capture-range': 'cudaProfilerApi', 'capture-range-end': None, 'kill': 'none'}}}}, 'ray_kwargs': {'ray_init': {'num_cpus': None}, 'timeline_json_file': None}}
[36m(train_multi_agents pid=2477709)[0m WARNING: val_batch_size is deprecated. Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves.
[36m(train_multi_agents pid=2477709)[0m [validate_config] All configuration checks passed successfully!
[36m(train_multi_agents pid=2477709)[0m Using dataset class: RLHFDataset
[36m(train_multi_agents pid=2477709)[0m Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 256 examples [00:00, 70696.72 examples/s]
[36m(train_multi_agents pid=2477709)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(train_multi_agents pid=2477709)[0m WARNING:2025-08-23 22:26:30,228:Waiting for register center actor d3ASWH_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(pid=2491113)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(pid=2491470)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(pid=2491472)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(WorkerDict pid=2491471)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2491471)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2491471)[0m Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
[36m(WorkerDict pid=2491471)[0m Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:07<00:14,  7.10s/it]
[36m(pid=2491471)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(WorkerDict pid=2491113)[0m Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3Model is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=2491113)[0m Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491471)[0m Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.96s/it][32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=2491471)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  3.85s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.71s/it]
[36m(WorkerDict pid=2491471)[0m libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
[36m(WorkerDict pid=2491470)[0m Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:14<00:07,  7.31s/it][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491470)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.05s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.93s/it][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491472)[0m libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
[36m(WorkerDict pid=2491113)[0m libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
[36m(WorkerDict pid=2491471)[0m Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
[36m(WorkerDict pid=2491470)[0m libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory
[36m(train_multi_agents pid=2477709)[0m dataset len: 256
[36m(train_multi_agents pid=2477709)[0m Using dataset class: RLHFDataset
[36m(train_multi_agents pid=2477709)[0m dataset len: 256
[36m(train_multi_agents pid=2477709)[0m Size of train dataloader: 16, Size of val dataloader: 32
[36m(train_multi_agents pid=2477709)[0m Total training steps: 480
[36m(train_multi_agents pid=2477709)[0m [92mPPO trainer created for model: code_generator_model[0m
[36m(train_multi_agents pid=2477709)[0m [92mthe number of ppo_trainer_dict: 1[0m
[36m(train_multi_agents pid=2477709)[0m [96mNumber of PPO trainers: 1[0m
[36m(train_multi_agents pid=2477709)[0m [96mNumber of agent mappings: 2[0m
[36m(train_multi_agents pid=2477709)[0m [96mInitializing workers for all PPO trainers...[0m
[36m(train_multi_agents pid=2477709)[0m [92mInitializing workers for trainer: code_generator_model[0m
[36m(train_multi_agents pid=2477709)[0m colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
[36m(train_multi_agents pid=2477709)[0m bind role actor_rollout method chat_completion to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(train_multi_agents pid=2477709)[0m bind role actor_rollout method execute_method to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(train_multi_agents pid=2477709)[0m bind role actor_rollout method generate to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(train_multi_agents pid=2477709)[0m bind role actor_rollout method get_zeromq_address to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(train_multi_agents pid=2477709)[0m bind role actor_rollout method sleep to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(train_multi_agents pid=2477709)[0m bind role actor_rollout method wake_up to class <class 'verl.single_controller.ray.base.create_colocated_worker_cls.<locals>.WorkerDict'>
[36m(WorkerDict pid=2491113)[0m Model config after override: Qwen3Config {
[36m(WorkerDict pid=2491113)[0m   "architectures": [
[36m(WorkerDict pid=2491113)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=2491113)[0m   ],
[36m(WorkerDict pid=2491113)[0m   "attention_bias": false,
[36m(WorkerDict pid=2491113)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2491113)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2491113)[0m   "head_dim": 128,
[36m(WorkerDict pid=2491113)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2491113)[0m   "hidden_size": 2560,
[36m(WorkerDict pid=2491113)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2491113)[0m   "intermediate_size": 9728,
[36m(WorkerDict pid=2491113)[0m   "layer_types": [
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention"
[36m(WorkerDict pid=2491113)[0m   ],
[36m(WorkerDict pid=2491113)[0m   "max_position_embeddings": 40960,
[36m(WorkerDict pid=2491113)[0m   "max_window_layers": 36,
[36m(WorkerDict pid=2491113)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=2491113)[0m   "num_attention_heads": 32,
[36m(WorkerDict pid=2491113)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=2491113)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=2491113)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2491113)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2491113)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2491113)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=2491113)[0m   "sliding_window": null,
[36m(WorkerDict pid=2491113)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2491113)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2491113)[0m   "transformers_version": "4.55.2",
[36m(WorkerDict pid=2491113)[0m   "use_cache": true,
[36m(WorkerDict pid=2491113)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2491113)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2491113)[0m }
[36m(WorkerDict pid=2491113)[0m 
[36m(WorkerDict pid=2491472)[0m Skipping monkey patch for Qwen3ForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=2491113)[0m Qwen3ForCausalLM contains 4.02B parameters
[36m(WorkerDict pid=2491113)[0m wrap_policy: functools.partial(<function _or_policy at 0x703f80e56660>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x703f80e56520>, transformer_layer_cls={<class 'transformers.models.qwen3.modeling_qwen3.Qwen3DecoderLayer'>})])
[36m(WorkerDict pid=2491113)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=2491113)[0m Actor use_fused_kernels=False
[36m(WorkerDict pid=2491470)[0m Skipping monkey patch for Qwen3ForCausalLM as use_fused_kernels is False or fused_kernels_backend is None[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491113)[0m Model config after override: Qwen3Config {
[36m(WorkerDict pid=2491113)[0m   "architectures": [
[36m(WorkerDict pid=2491113)[0m     "Qwen3ForCausalLM"
[36m(WorkerDict pid=2491113)[0m   ],
[36m(WorkerDict pid=2491113)[0m   "attention_bias": false,
[36m(WorkerDict pid=2491113)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2491113)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2491113)[0m   "head_dim": 128,
[36m(WorkerDict pid=2491113)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2491113)[0m   "hidden_size": 2560,
[36m(WorkerDict pid=2491113)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2491113)[0m   "intermediate_size": 9728,
[36m(WorkerDict pid=2491113)[0m   "layer_types": [
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:06<00:12,  6.25s/it]
[36m(WorkerDict pid=2491113)[0m Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491113)[0m Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.03s/it][32m [repeated 4x across cluster][0m
[36m(WorkerDict pid=2491113)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  3.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00,  4.09s/it]
[36m(WorkerDict pid=2491472)[0m /home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2491472)[0m   warnings.warn(
[36m(WorkerDict pid=2491472)[0m Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.69s/it][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491472)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  3.71s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:13<00:00,  4.52s/it][32m [repeated 3x across cluster][0m
[36m(AsyncvLLMServer pid=2506130)[0m Using blocking ray.get inside async actor. This blocks the event loop. Please use `await` on object ref with asyncio.gather if you want to yield execution to the event loop instead.
[36m(WorkerDict pid=2491113)[0m /home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491113)[0m   warnings.warn([32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention",
[36m(WorkerDict pid=2491113)[0m     "full_attention"
[36m(WorkerDict pid=2491113)[0m   ],
[36m(WorkerDict pid=2491113)[0m   "max_position_embeddings": 40960,
[36m(WorkerDict pid=2491113)[0m   "max_window_layers": 36,
[36m(WorkerDict pid=2491113)[0m   "model_type": "qwen3",
[36m(WorkerDict pid=2491113)[0m   "num_attention_heads": 32,
[36m(WorkerDict pid=2491113)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=2491113)[0m   "num_key_value_heads": 8,
[36m(WorkerDict pid=2491113)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2491113)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2491113)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2491113)[0m   "rope_theta": 1000000,
[36m(WorkerDict pid=2491113)[0m   "sliding_window": null,
[36m(WorkerDict pid=2491113)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2491113)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2491113)[0m   "transformers_version": "4.55.2",
[36m(WorkerDict pid=2491113)[0m   "use_cache": true,
[36m(WorkerDict pid=2491113)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2491113)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2491113)[0m }
[36m(WorkerDict pid=2491113)[0m 
[36m(WorkerDict pid=2491113)[0m Skipping monkey patch for Qwen3ForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=2491470)[0m Skipping monkey patch for Qwen3ForCausalLM as use_fused_kernels is False or fused_kernels_backend is None
[36m(WorkerDict pid=2491113)[0m Qwen3ForCausalLM contains 4.02B parameters
[36m(WorkerDict pid=2491113)[0m wrap_policy: functools.partial(<function _or_policy at 0x703f80e56660>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x703f80e56520>, transformer_layer_cls={<class 'transformers.models.qwen3.modeling_qwen3.Qwen3DecoderLayer'>})])
[36m(WorkerDict pid=2491113)[0m Total steps: 480, num_warmup_steps: 0
[36m(WorkerDict pid=2491113)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=2491113)[0m Actor use_fused_kernels=False
[36m(WorkerDict pid=2491471)[0m Skipping monkey patch for Qwen3ForCausalLM as use_fused_kernels is False or fused_kernels_backend is None[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2491113)[0m INFO 08-23 22:37:49 [__init__.py:235] Automatically detected platform cuda.
[36m(pid=2506130)[0m INFO 08-23 22:38:01 [__init__.py:235] Automatically detected platform cuda.[32m [repeated 5x across cluster][0m
[36m(AsyncvLLMServer pid=2506130)[0m FastAPI listen on 169.228.33.163:49379
[36m(AsyncvLLMServer pid=2506130)[0m override_generation_config: {'n': 4, 'logprobs': 0, 'repetition_penalty': 1.0, 'max_new_tokens': 8192, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:38:12 [config.py:1604] Using max model len 8192
[36m(AsyncvLLMServer pid=2506130)[0m WARNING 08-23 22:38:12 [arg_utils.py:1695] Detected VLLM_USE_V1=1 with Engine in background thread. Usage should be considered experimental. Please report any issues on Github.
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:38:12 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=1048576.
[36m(AsyncvLLMServer pid=2506130)[0m instance_id: 66abc2a5-c43b-422f-b276-3df7e6fade5d:d3ASWH:1:0 initializes with external actors: ['d3ASWHWorkerDict_0:0', 'd3ASWHWorkerDict_0:1', 'd3ASWHWorkerDict_0:2', 'd3ASWHWorkerDict_0:3']
[36m(AsyncvLLMServer pid=2506130)[0m VERL_VLLM_ZMQ_ADDRESSES: ['ipc:///tmp/verl_vllm_zmq_2491113.ipc', 'ipc:///tmp/verl_vllm_zmq_2491470.ipc', 'ipc:///tmp/verl_vllm_zmq_2491471.ipc', 'ipc:///tmp/verl_vllm_zmq_2491472.ipc']
[36m(AsyncvLLMServer pid=2506130)[0m WARNING 08-23 22:38:14 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: In a Ray actor and can only be spawned
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:38:21 [__init__.py:235] Automatically detected platform cuda.
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:38:23 [core.py:572] Waiting for init message from front-end.
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:38:23 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='Qwen/Qwen3-4B', speculative_config=None, tokenizer='Qwen/Qwen3-4B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-4B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":0,"local_cache_dir":null}
[36m(WorkerDict pid=2491113)[0m INFO 08-23 22:38:24 [__init__.py:1375] Found nccl from library libnccl.so.2
[36m(WorkerDict pid=2491113)[0m INFO 08-23 22:38:24 [pynccl.py:70] vLLM is using nccl==2.26.2
[36m(WorkerDict pid=2491113)[0m INFO 08-23 22:47:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2d2dd0bf'), local_subscribe_addr='ipc:///tmp/f1b9215d-afc7-4af9-9141-9be5fc45589a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[36m(WorkerDict pid=2491113)[0m INFO 08-23 22:47:02 [parallel_state.py:1102] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[36m(WorkerDict pid=2491113)[0m WARNING 08-23 22:47:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[36m(WorkerDict pid=2491113)[0m Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
[36m(WorkerDict pid=2491113)[0m Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:00<00:00,  3.30it/s]
[36m(WorkerDict pid=2491113)[0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.46it/s]
[36m(WorkerDict pid=2491113)[0m Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:01<00:00,  2.59it/s]
[36m(WorkerDict pid=2491113)[0m 
[36m(train_multi_agents pid=2477709)[0m wandb: Currently logged in as: yuz285 (yuz285-uc-san-diego) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(train_multi_agents pid=2477709)[0m wandb: creating run
[36m(train_multi_agents pid=2477709)[0m wandb: Tracking run with wandb version 0.21.1
[36m(train_multi_agents pid=2477709)[0m wandb: Run data is saved locally in /home/lah003/workspace/PettingLLMs/wandb/run-20250823_225613-z8psqfil
[36m(train_multi_agents pid=2477709)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(train_multi_agents pid=2477709)[0m wandb: Syncing run code_test
[36m(train_multi_agents pid=2477709)[0m wandb: â­ï¸ View project at https://wandb.ai/yuz285-uc-san-diego/pettingllms
[36m(train_multi_agents pid=2477709)[0m wandb: ðŸš€ View run at https://wandb.ai/yuz285-uc-san-diego/pettingllms/runs/z8psqfil
[36m(train_multi_agents pid=2477709)[0m Training Progress:   0%|          | 0/480 [00:00<?, ?it/s]Epochs:   0%|          | 0/30 [00:00<?, ?it/s]INFO:2025-08-23 22:56:20,044:{
[36m(train_multi_agents pid=2477709)[0m   "env_idx": -1,
[36m(train_multi_agents pid=2477709)[0m   "rollout_idx": -1,
[36m(train_multi_agents pid=2477709)[0m   "agent_rewards": "env_workers",
[36m(train_multi_agents pid=2477709)[0m   "termination_reason": "init 128 env workers",
[36m(train_multi_agents pid=2477709)[0m   "timestamp": "2025-08-23T22:56:20.044204",
[36m(train_multi_agents pid=2477709)[0m   "extra_data": {}
[36m(train_multi_agents pid=2477709)[0m }
[36m(WorkerDict pid=2491471)[0m INFO 08-23 22:38:24 [__init__.py:1375] Found nccl from library libnccl.so.2[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491471)[0m INFO 08-23 22:38:24 [pynccl.py:70] vLLM is using nccl==2.26.2[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491113)[0m INFO 08-23 22:47:02 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen3-4B...
[36m(WorkerDict pid=2491471)[0m INFO 08-23 22:47:02 [gpu_model_runner.py:1875] Loading model from scratch...
[36m(WorkerDict pid=2491471)[0m INFO 08-23 22:47:02 [cuda.py:259] Using Flash Attention backend on V1 engine.
[36m(WorkerDict pid=2491472)[0m INFO 08-23 22:47:02 [weight_utils.py:296] Using model weights format ['*.safetensors']
[36m(WorkerDict pid=2491471)[0m INFO 08-23 22:47:04 [default_loader.py:262] Loading weights took 1.20 seconds
[36m(WorkerDict pid=2491471)[0m INFO 08-23 22:47:04 [gpu_model_runner.py:1892] Model loading took 1.8871 GiB and 1.793406 seconds
[36m(WorkerDict pid=2491471)[0m INFO 08-23 22:55:58 [gpu_worker.py:255] Available KV cache memory: 100.21 GiB
[36m(WorkerDict pid=2491471)[0m INFO 08-23 22:47:02 [parallel_state.py:1102] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491471)[0m WARNING 08-23 22:47:02 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491471)[0m INFO 08-23 22:47:02 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen3-4B...[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491472)[0m INFO 08-23 22:47:02 [gpu_model_runner.py:1875] Loading model from scratch...[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491472)[0m INFO 08-23 22:47:02 [cuda.py:259] Using Flash Attention backend on V1 engine.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491470)[0m INFO 08-23 22:47:02 [weight_utils.py:296] Using model weights format ['*.safetensors'][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491470)[0m INFO 08-23 22:47:04 [default_loader.py:262] Loading weights took 1.27 seconds[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2491470)[0m INFO 08-23 22:47:05 [gpu_model_runner.py:1892] Model loading took 1.8875 GiB and 2.423662 seconds[32m [repeated 3x across cluster][0m
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:55:59 [kv_cache_utils.py:833] GPU KV cache size: 2,918,784 tokens
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:55:59 [kv_cache_utils.py:837] Maximum concurrency for 8,192 tokens per request: 356.30x
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:55:59 [kv_cache_utils.py:833] GPU KV cache size: 2,918,928 tokens
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:55:59 [kv_cache_utils.py:837] Maximum concurrency for 8,192 tokens per request: 356.31x
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:55:59 [kv_cache_utils.py:833] GPU KV cache size: 2,918,944 tokens
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:55:59 [kv_cache_utils.py:837] Maximum concurrency for 8,192 tokens per request: 356.32x
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:55:59 [kv_cache_utils.py:833] GPU KV cache size: 2,918,880 tokens
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:55:59 [kv_cache_utils.py:837] Maximum concurrency for 8,192 tokens per request: 356.31x
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:55:59 [core.py:193] init engine (profile, create kv cache, warmup model) took 533.86 seconds
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:56:01 [loggers.py:141] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 182424
[36m(AsyncvLLMServer pid=2506130)[0m WARNING 08-23 22:56:01 [config.py:1528] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:56:01 [serving_chat.py:122] Using default chat sampling params from model: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': -1, 'top_p': 1, 'max_tokens': 8192}
[36m(AsyncvLLMServer pid=2506130)[0m INFO 08-23 22:56:12 [block_pool.py:321] Successfully reset prefix cache
[36m(WorkerDict pid=2491470)[0m INFO 08-23 22:55:59 [gpu_worker.py:255] Available KV cache memory: 100.21 GiB[32m [repeated 3x across cluster][0m
[36m(train_multi_agents pid=2477709)[0m [92mAll workers initialized successfully[0m
[36m(train_multi_agents pid=2477709)[0m DEBUG: unready_dp_ranks={0}
[36m(train_multi_agents pid=2477709)[0m DEBUG: Processing unready_dp_ranks: {0}
[36m(train_multi_agents pid=2477709)[0m DEBUG: Creating servers for worker_group case
[36m(train_multi_agents pid=2477709)[0m DEBUG: Trying to reuse existing async_llm_server_{rank} actors
[36m(train_multi_agents pid=2477709)[0m DEBUG: Reused existing server actor 'async_llm_server_0': Actor(AsyncvLLMServer, d5b31a3e2451b42748ad762101000000)
[36m(train_multi_agents pid=2477709)[0m DEBUG: Processing server for rank 0
[36m(train_multi_agents pid=2477709)[0m DEBUG: Getting server address for rank 0
[36m(train_multi_agents pid=2477709)[0m DEBUG: Got address 169.228.33.163:49379 for rank 0
[36m(train_multi_agents pid=2477709)[0m DEBUG: Successfully initialized server for rank 0
[36m(train_multi_agents pid=2477709)[0m DEBUG: Returning 1 servers and 1 addresses
[36m(train_multi_agents pid=2477709)[0m [92mSuccessfully initialized 1 LLM servers for model: code_generator_model[0m
[36m(train_multi_agents pid=2477709)[0m env_name: code_env
[36m(train_multi_agents pid=2477709)[0m sample_num: 4
[36m(train_multi_agents pid=2477709)[0m sample_num_list: []
[36m(train_multi_agents pid=2477709)[0m agent_config_dict keys: ['code_generator', 'test_generator']
[36m(train_multi_agents pid=2477709)[0m [96mLoading checkpoint for trainer code_generator_model[0m
[36m(train_multi_agents pid=2477709)[0m Checkpoint tracker file does not exist: /home/lah003/workspace/PettingLLMs/checkpoints/verl_examples/gsm8k/latest_checkpointed_iteration.txt
[36m(train_multi_agents pid=2477709)[0m Training from scratch
[36m(train_multi_agents pid=2477709)[0m Time taken to validate agent_code_generator_model: 1.6689300537109375e-06
[36m(train_multi_agents pid=2477709)[0m ðŸ”„ Loading 32 problems from dataset train...
[36m(train_multi_agents pid=2477709)[0m ðŸ“ Loading from local dataset: /home/lah003/workspace/PettingLLMs/datasets/train
[36m(train_multi_agents pid=2477709)[0m âœ… Successfully loaded local dataset with 4096 samples
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 1/32 (index=162)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 2/32 (index=2677)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 3/32 (index=2786)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 4/32 (index=3871)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 5/32 (index=3298)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 6/32 (index=3451)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 7/32 (index=237)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 8/32 (index=2021)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 9/32 (index=377)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 10/32 (index=2489)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 11/32 (index=476)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 12/32 (index=1577)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 13/32 (index=2460)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 14/32 (index=1489)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 15/32 (index=3628)
[36m(train_multi_agents pid=2477709)[0m Epoch 0 (Step 1):   0%|          | 0/30 [00:00<?, ?it/s]
[36m(train_multi_agents pid=2477709)[0m 
[36m(train_multi_agents pid=2477709)[0m 
[36m(train_multi_agents pid=2477709)[0m Updating Parameters:   0%|          | 0/1 [00:00<?, ?it/s][A[A
[36m(train_multi_agents pid=2477709)[0m 
[36m(train_multi_agents pid=2477709)[0m Updating code_generator_model:   0%|          | 0/1 [00:00<?, ?it/s][A[A
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 16/32 (index=3200)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 17/32 (index=3703)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 18/32 (index=1195)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 19/32 (index=2082)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 20/32 (index=2984)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 21/32 (index=123)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 22/32 (index=3302)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 23/32 (index=556)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 24/32 (index=701)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 25/32 (index=4092)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 26/32 (index=1820)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 27/32 (index=3516)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 28/32 (index=1041)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 29/32 (index=1082)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 30/32 (index=1447)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 31/32 (index=238)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 32/32 (index=3671)
[36m(train_multi_agents pid=2477709)[0m âœ… Successfully loaded 32 train problems
[36m(train_multi_agents pid=2477709)[0m ðŸ”„ Loading 32 problems from dataset train...
[36m(train_multi_agents pid=2477709)[0m ðŸ“ Loading from local dataset: /home/lah003/workspace/PettingLLMs/datasets/train
[36m(train_multi_agents pid=2477709)[0m âœ… Successfully loaded local dataset with 4096 samples
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 1/32 (index=966)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 2/32 (index=499)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 3/32 (index=2645)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 4/32 (index=151)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 5/32 (index=4093)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 6/32 (index=872)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 7/32 (index=2195)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 8/32 (index=982)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 9/32 (index=2826)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 10/32 (index=2524)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 11/32 (index=2641)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 12/32 (index=3687)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 13/32 (index=2241)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 14/32 (index=2830)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 15/32 (index=3204)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 16/32 (index=3370)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 17/32 (index=1852)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 18/32 (index=466)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 19/32 (index=1504)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 20/32 (index=3860)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 21/32 (index=866)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 22/32 (index=2246)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 23/32 (index=3045)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 24/32 (index=1865)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 25/32 (index=321)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 26/32 (index=3788)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 27/32 (index=1367)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 28/32 (index=760)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 29/32 (index=3310)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 30/32 (index=2215)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 31/32 (index=3246)
[36m(train_multi_agents pid=2477709)[0m âœ… Loaded train problem 32/32 (index=1661)
[36m(train_multi_agents pid=2477709)[0m âœ… Successfully loaded 32 train problems
[36m(train_multi_agents pid=2477709)[0m 'epoch 0, step 1 started'
[36m(train_multi_agents pid=2477709)[0m length of loaded batch: 30
[36m(train_multi_agents pid=2477709)[0m [93mLoaded cached batch for code_generator_model from logs/batch_cache/epoch0_code_generator_model.pt[0m
[36m(AgentLoopWorker pid=2529691)[0m INFO 08-23 22:56:24 [__init__.py:235] Automatically detected platform cuda.
[36m(AgentLoopWorker pid=2529689)[0m INFO 08-23 22:56:30 [__init__.py:235] Automatically detected platform cuda.[32m [repeated 3x across cluster][0m
[36m(AgentLoopWorker pid=2529766)[0m INFO 08-23 22:56:36 [__init__.py:235] Automatically detected platform cuda.[32m [repeated 5x across cluster][0m
[36m(train_multi_agents pid=2477709)[0m Training failed in train_multi_agents: [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=2491113, ip=169.228.33.163, actor_id=356d8ebe12bb936e7931122701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x703d272cc890>)
[36m(train_multi_agents pid=2477709)[0m   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[36m(train_multi_agents pid=2477709)[0m     return self.__get_result()
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[36m(train_multi_agents pid=2477709)[0m     raise self._exception
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/ray/base.py", line 720, in func
[36m(train_multi_agents pid=2477709)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/base/decorator.py", line 514, in inner
[36m(train_multi_agents pid=2477709)[0m     return func(*args, **kwargs)
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/fsdp_workers.py", line 779, in compute_log_prob
[36m(train_multi_agents pid=2477709)[0m     output, entropys = self.actor.compute_log_prob(data=data, calculate_entropy=True)
[36m(train_multi_agents pid=2477709)[0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 105, in f
[36m(train_multi_agents pid=2477709)[0m     return self.log(decorated_function, *args, **kwargs)
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 118, in log
[36m(train_multi_agents pid=2477709)[0m     output = func(*args, **kwargs)
[36m(train_multi_agents pid=2477709)[0m              ^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m 
[36m(train_multi_agents pid=2477709)[0m 
[36m(train_multi_agents pid=2477709)[0m                                                                     [A[AEpoch 0 (Step 1):   0%|          | 0/30 [00:18<?, ?it/s]
[36m(train_multi_agents pid=2477709)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=2491471, ip=169.228.33.163, actor_id=f7c4d5b94749a68b39834e8901000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7a7473ecc710>)
[36m(train_multi_agents pid=2477709)[0m   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
[36m(train_multi_agents pid=2477709)[0m     return self.__get_result()
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
[36m(train_multi_agents pid=2477709)[0m     raise self._exception
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/ray/base.py", line 720, in func
[36m(train_multi_agents pid=2477709)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/base/decorator.py", line 514, in inner
[36m(train_multi_agents pid=2477709)[0m     return func(*args, **kwargs)
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/fsdp_workers.py", line 779, in compute_log_prob
[36m(train_multi_agents pid=2477709)[0m     output, entropys = self.actor.compute_log_prob(data=data, calculate_entropy=True)
[36m(train_multi_agents pid=2477709)[0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 105, in f
[36m(train_multi_agents pid=2477709)[0m     return self.log(decorated_function, *args, **kwargs)
[36m(train_multi_agents pid=2477709)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 118, in log
[36m(train_multi_agents pid=2477709)[0m     output = func(*args, **kwargs)
[36m(train_multi_agents pid=2477709)[0m              ^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 357, in compute_log_prob
[36m(train_multi_agents pid=2477709)[0m     entropy, log_probs = self._forward_micro_batch(
[36m(train_multi_agents pid=2477709)[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 267, in _forward_micro_batch
[36m(train_multi_agents pid=2477709)[0m     entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)
[36m(train_multi_agents pid=2477709)[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/torch_functional.py", line 147, in entropy_from_logits
[36m(train_multi_agents pid=2477709)[0m     pd = torch.nn.functional.softmax(logits, dim=-1)
[36m(train_multi_agents pid=2477709)[0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/torch/nn/functional.py", line 2140, in softmax
[36m(train_multi_agents pid=2477709)[0m     ret = input.softmax(dim)
[36m(train_multi_agents pid=2477709)[0m           ^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 37.09 GiB. GPU 0 has a total capacity of 178.36 GiB of which 5.59 GiB is free. Including non-PyTorch memory, this process has 172.76 GiB memory in use. Of the allocated memory 169.57 GiB is allocated by PyTorch, and 102.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(train_multi_agents pid=2477709)[0m wandb:                                                                                
[36m(train_multi_agents pid=2477709)[0m wandb: ðŸš€ View run code_test at: https://wandb.ai/yuz285-uc-san-diego/pettingllms/runs/z8psqfil
[36m(train_multi_agents pid=2477709)[0m wandb: â­ï¸ View project at: https://wandb.ai/yuz285-uc-san-diego/pettingllms
[36m(train_multi_agents pid=2477709)[0m wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[36m(train_multi_agents pid=2477709)[0m wandb: Find logs at: ./wandb/run-20250823_225613-z8psqfil/logs
[36m(train_multi_agents pid=2477709)[0m Training Progress:   0%|          | 0/480 [00:19<?, ?it/s]
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 357, in compute_log_prob
[36m(train_multi_agents pid=2477709)[0m     entropy, log_probs = self._forward_micro_batch(
[36m(train_multi_agents pid=2477709)[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 267, in _forward_micro_batch
[36m(train_multi_agents pid=2477709)[0m     entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)
[36m(train_multi_agents pid=2477709)[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/torch_functional.py", line 147, in entropy_from_logits
[36m(train_multi_agents pid=2477709)[0m     pd = torch.nn.functional.softmax(logits, dim=-1)
[36m(train_multi_agents pid=2477709)[0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m   File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/torch/nn/functional.py", line 2140, in softmax
[36m(train_multi_agents pid=2477709)[0m     ret = input.softmax(dim)
[36m(train_multi_agents pid=2477709)[0m           ^^^^^^^^^^^^^^^^^^
[36m(train_multi_agents pid=2477709)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 37.09 GiB. GPU 0 has a total capacity of 178.36 GiB of which 5.56 GiB is free. Including non-PyTorch memory, this process has 172.79 GiB memory in use. Of the allocated memory 169.58 GiB is allocated by PyTorch, and 130.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(train_multi_agents pid=2477709)[0m [93mCleaning up LLM servers...[0m
[36m(train_multi_agents pid=2477709)[0m [93mKilled LLM server: Actor(AsyncvLLMServer, d5b31a3e2451b42748ad762101000000)[0m
[36m(train_multi_agents pid=2477709)[0m [93mCleaned up trainer for model: code_generator_model[0m
[36m(train_multi_agents pid=2477709)[0m [92mMulti-agent trainer cleanup completed[0m
[36m(train_multi_agents pid=2477709)[0m Executing final cleanup in train_multi_agents...
[36m(train_multi_agents pid=2477709)[0m [93mCleaned up trainer for model: code_generator_model[0m
[36m(train_multi_agents pid=2477709)[0m [92mMulti-agent trainer cleanup completed[0m
Training failed with error: [36mray::train_multi_agents()[39m (pid=2477709, ip=169.228.33.163)
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 282, in train_multi_agents
    raise e
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 272, in train_multi_agents
    trainer.fit()
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/multi_agents_ppo_trainer.py", line 640, in fit
    self._update_parameters(
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/multi_agents_ppo_trainer.py", line 367, in _update_parameters
    old_log_prob = ppo_trainer.actor_rollout_wg.compute_log_prob(batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/ray/base.py", line 50, in __call__
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=2491113, ip=169.228.33.163, actor_id=356d8ebe12bb936e7931122701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x703d272cc890>)
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
           ^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/ray/base.py", line 720, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/base/decorator.py", line 514, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/fsdp_workers.py", line 779, in compute_log_prob
    output, entropys = self.actor.compute_log_prob(data=data, calculate_entropy=True)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 105, in f
    return self.log(decorated_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 118, in log
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 357, in compute_log_prob
    entropy, log_probs = self._forward_micro_batch(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 267, in _forward_micro_batch
    entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/torch_functional.py", line 147, in entropy_from_logits
    pd = torch.nn.functional.softmax(logits, dim=-1)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/torch/nn/functional.py", line 2140, in softmax
    ret = input.softmax(dim)
          ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 37.09 GiB. GPU 0 has a total capacity of 178.36 GiB of which 5.56 GiB is free. Including non-PyTorch memory, this process has 172.79 GiB memory in use. Of the allocated memory 169.58 GiB is allocated by PyTorch, and 130.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Cleaning up Ray cluster due to error...

==================================================
STARTING RAY CLEANUP...
==================================================
Step 1: Attempting normal Ray shutdown...
âœ“ Normal Ray shutdown completed.
Step 2: Force killing Ray processes...
Force killing Ray processes...
Force killed all Ray processes
Step 3: Cleaning Ray environment...
Cleared 3 Ray environment variables
==================================================
RAY CLEANUP COMPLETED
==================================================
Executing cleanup in run_ppo...

==================================================
STARTING RAY CLEANUP...
==================================================
Ray is not initialized, but will force cleanup anyway...
Step 2: Force killing Ray processes...
Force killing Ray processes...
Force killed all Ray processes
Step 3: Cleaning Ray environment...
Cleared 0 Ray environment variables
==================================================
RAY CLEANUP COMPLETED
==================================================
Training failed with unexpected error: [36mray::train_multi_agents()[39m (pid=2477709, ip=169.228.33.163)
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 282, in train_multi_agents
    raise e
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 272, in train_multi_agents
    trainer.fit()
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/multi_agents_ppo_trainer.py", line 640, in fit
    self._update_parameters(
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/multi_agents_ppo_trainer.py", line 367, in _update_parameters
    old_log_prob = ppo_trainer.actor_rollout_wg.compute_log_prob(batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/ray/base.py", line 50, in __call__
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=2491113, ip=169.228.33.163, actor_id=356d8ebe12bb936e7931122701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x703d272cc890>)
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
           ^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/ray/base.py", line 720, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/base/decorator.py", line 514, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/fsdp_workers.py", line 779, in compute_log_prob
    output, entropys = self.actor.compute_log_prob(data=data, calculate_entropy=True)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 105, in f
    return self.log(decorated_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 118, in log
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 357, in compute_log_prob
    entropy, log_probs = self._forward_micro_batch(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 267, in _forward_micro_batch
    entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/torch_functional.py", line 147, in entropy_from_logits
    pd = torch.nn.functional.softmax(logits, dim=-1)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/torch/nn/functional.py", line 2140, in softmax
    ret = input.softmax(dim)
          ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 37.09 GiB. GPU 0 has a total capacity of 178.36 GiB of which 5.56 GiB is free. Including non-PyTorch memory, this process has 172.79 GiB memory in use. Of the allocated memory 169.58 GiB is allocated by PyTorch, and 130.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

==================================================
STARTING RAY CLEANUP...
==================================================
Ray is not initialized, but will force cleanup anyway...
Step 2: Force killing Ray processes...
Force killing Ray processes...
Force killed all Ray processes
Step 3: Cleaning Ray environment...
Cleared 0 Ray environment variables
==================================================
RAY CLEANUP COMPLETED
==================================================
Executing final cleanup in main...

==================================================
STARTING RAY CLEANUP...
==================================================
Ray is not initialized, but will force cleanup anyway...
Step 2: Force killing Ray processes...
Force killing Ray processes...
Force killed all Ray processes
Step 3: Cleaning Ray environment...
Cleared 0 Ray environment variables
==================================================
RAY CLEANUP COMPLETED
==================================================
Error executing job with overrides: ['models.model_0.ppo_trainer_config.algorithm.adv_estimator=grpo', 'models.model_0.ppo_trainer_config.actor_rollout_ref.actor.use_kl_loss=True', 'models.model_0.ppo_trainer_config.trainer.n_gpus_per_node=1', 'models.model_0.ppo_trainer_config.trainer.nnodes=1', '+models.model_0.ppo_trainer_config.data.train_files=/home/lah003/data/code/model_0/text/train.parquet', '+models.model_0.ppo_trainer_config.data.val_files=/home/lah003/data/code/model_0/text/test.parquet']
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 295, in <module>
    main()
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 140, in main
    raise e
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 132, in main
    run_ppo(config)
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 165, in run_ppo
    raise e
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 160, in run_ppo
    ray.get(train_multi_agents.remote(config))
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2858, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/ray/_private/worker.py", line 958, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::train_multi_agents()[39m (pid=2477709, ip=169.228.33.163)
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 282, in train_multi_agents
    raise e
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/train.py", line 272, in train_multi_agents
    trainer.fit()
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/multi_agents_ppo_trainer.py", line 640, in fit
    self._update_parameters(
  File "/home/lah003/workspace/PettingLLMs/pettingllms/trainer/multi_agents_ppo_trainer.py", line 367, in _update_parameters
    old_log_prob = ppo_trainer.actor_rollout_wg.compute_log_prob(batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/ray/base.py", line 50, in __call__
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_compute_log_prob()[39m (pid=2491113, ip=169.228.33.163, actor_id=356d8ebe12bb936e7931122701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x703d272cc890>)
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
           ^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/ray/base.py", line 720, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/single_controller/base/decorator.py", line 514, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/fsdp_workers.py", line 779, in compute_log_prob
    output, entropys = self.actor.compute_log_prob(data=data, calculate_entropy=True)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 105, in f
    return self.log(decorated_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/profiler/performance.py", line 118, in log
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 357, in compute_log_prob
    entropy, log_probs = self._forward_micro_batch(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/workers/actor/dp_actor.py", line 267, in _forward_micro_batch
    entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/verl/verl/utils/torch_functional.py", line 147, in entropy_from_logits
    pd = torch.nn.functional.softmax(logits, dim=-1)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lah003/workspace/PettingLLMs/pettingllms-venv/lib/python3.12/site-packages/torch/nn/functional.py", line 2140, in softmax
    ret = input.softmax(dim)
          ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 37.09 GiB. GPU 0 has a total capacity of 178.36 GiB of which 5.56 GiB is free. Including non-PyTorch memory, this process has 172.79 GiB memory in use. Of the allocated memory 169.58 GiB is allocated by PyTorch, and 130.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

==================================================
STARTING RAY CLEANUP...
==================================================
Ray is not initialized, but will force cleanup anyway...
Step 2: Force killing Ray processes...
Force killing Ray processes...
Force killed all Ray processes
Step 3: Cleaning Ray environment...
Cleared 0 Ray environment variables
==================================================
RAY CLEANUP COMPLETED
==================================================

==================================================
STARTING RAY CLEANUP...
==================================================
Ray is not initialized, but will force cleanup anyway...
Step 2: Force killing Ray processes...
Force killing Ray processes...
Force killed all Ray processes
Step 3: Cleaning Ray environment...
Cleared 0 Ray environment variables
==================================================
RAY CLEANUP COMPLETED
==================================================
