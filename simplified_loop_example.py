#!/usr/bin/env python3
"""
ç¤ºä¾‹ï¼šä½¿ç”¨ç®€åŒ–çš„multi-agent loopå‡½æ•°

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†æ–°çš„è®¾è®¡ï¼š
1. loop()å‡½æ•°åªå¤„ç†å•æ­¥äº¤äº’
2. è¿”å›æœ€åŸºæœ¬çš„æ•°æ®ï¼šprompt, response, action, reward
3. å¾ªç¯æ§åˆ¶åœ¨å¤–éƒ¨å¤„ç†ï¼ˆå¦‚åœ¨engineä¸­ï¼‰
"""

import asyncio
import tempfile
from typing import Dict, Any

from pettingllms.agentgpraphs.design_human_interact.agent_collaboration_graph import FrontendDesignAgentGraph
from pettingllms.agentgpraphs.design_human_interact.websight_env import WebEnv


async def demonstrate_simplified_loop():
    """æ¼”ç¤ºç®€åŒ–çš„loopå‡½æ•°ä½¿ç”¨"""
    print("ğŸš€ æ¼”ç¤ºç®€åŒ–çš„Multi-Agent Loopè®¾è®¡")
    print("=" * 50)
    
    # åˆ›å»ºagent graph
    agent_graph = FrontendDesignAgentGraph(
        hostname="localhost",
        code_port=8000,
        visual_port=8001,
        max_iterations=3,
        temp_path=tempfile.gettempdir()
    )
    
    # åˆ›å»ºmockç¯å¢ƒå’Œè§‚å¯Ÿ
    mock_sample = {
        "task_id": "demo_task",
        "problem_description": "Create a navigation bar",
        "ground_truth": "<html><nav>Navigation</nav></html>"
    }
    
    env = WebEnv(task=mock_sample, max_turns=6, temp_path=tempfile.gettempdir())
    
    # é‡ç½®agents
    agents_info = agent_graph._get_agents_list()
    print(f"ğŸ” æ£€æµ‹åˆ° {len(agents_info)} ä¸ªagents: {[name for name, _, _ in agents_info]}")
    
    for _, agent_instance, _ in agents_info:
        agent_instance.reset()
    
    # è·å–åˆå§‹è§‚å¯Ÿ
    obs, _ = env.reset()
    
    print(f"\nğŸ“ ä»»åŠ¡: {mock_sample['problem_description']}")
    
    # æ¨¡æ‹Ÿengineä¸­çš„å¤šæ­¥å¾ªç¯
    for step_idx in range(3):
        print(f"\nğŸ”„ Step {step_idx + 1}")
        print("-" * 30)
        
        # ============ è°ƒç”¨ç®€åŒ–çš„loopå‡½æ•° ============
        print("ğŸ“ è°ƒç”¨ agent_graph.loop()...")
        step_data = await agent_graph.loop(obs, step_idx)
        
        # æ˜¾ç¤ºloopå‡½æ•°çš„è¾“å‡º
        print("ğŸ“Š Loopå‡½æ•°è¾“å‡º:")
        for agent_name, agent_data in step_data.items():
            print(f"  {agent_name} ({agent_data['original_name']}):")
            print(f"    â†³ Action Type: {agent_data['action_type']}")
            print(f"    â†³ Action: {agent_data['action'][:50]}...")
            print(f"    â†³ Response: {agent_data['response'][:50]}...")
        
        # ============ ç¯å¢ƒäº¤äº’ (åœ¨engineä¸­å¤„ç†) ============
        print("\nğŸŒ ç¯å¢ƒäº¤äº’:")
        
        for agent_name, agent_data in step_data.items():
            action_type = agent_data["action_type"]
            action = agent_data["action"]
            
            print(f"  {agent_name}: {action_type} action")
            
            # ä¸ç¯å¢ƒäº¤äº’
            obs, reward, done, info = env.step(action_type, action)
            
            print(f"    â†³ Reward: {reward}")
            
            # æ›´æ–°å¥–åŠ±
            env_results = {"default_reward": reward}
            step_data = await agent_graph.update_rewards(step_data, env_results)
        
        # ============ æ˜¾ç¤ºæ›´æ–°åçš„æ•°æ® ============
        print("\nâœ… æ›´æ–°åçš„æ•°æ®:")
        for agent_name, agent_data in step_data.items():
            print(f"  {agent_name}: reward = {agent_data['reward']}")
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        if done or not obs.get("current_image"):
            print("ğŸ è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶")
            break
    
    print("\nâœ¨ æ¼”ç¤ºå®Œæˆ!")
    env.cleanup()


def show_data_structure():
    """å±•ç¤ºç®€åŒ–çš„æ•°æ®ç»“æ„"""
    print("\nğŸ“‹ ç®€åŒ–çš„æ•°æ®ç»“æ„:")
    print("=" * 40)
    
    example_loop_output = {
        "agent1": {
            "original_name": "code_agent",
            "prompt": [{"role": "user", "content": "Generate HTML..."}],
            "response": "Generated HTML response...",
            "action": "<html><body>Generated content</body></html>",
            "action_type": "code",
            "reward": 0.8
        },
        "agent2": {
            "original_name": "visual_agent", 
            "prompt": [{"role": "user", "content": "Analyze design..."}],
            "response": "Visual analysis response...",
            "action": "Add more padding and improve colors",
            "action_type": "visual",
            "reward": 0.6
        }
    }
    
    import json
    print(json.dumps(example_loop_output, indent=2, ensure_ascii=False))


def compare_old_vs_new():
    """å¯¹æ¯”æ—§è®¾è®¡ä¸æ–°è®¾è®¡"""
    print("\nğŸ“Š è®¾è®¡å¯¹æ¯”:")
    print("=" * 40)
    
    comparison = """
    æ—§è®¾è®¡ (multi_agent_loop):
    âŒ å¤„ç†å®Œæ•´çš„å¤šæ­¥å¾ªç¯
    âŒ å¤æ‚çš„æ•°æ®ç»“æ„ (trajectory_steps, execution_timeç­‰)
    âŒ åŒ…å«æ—¶é—´ç»Ÿè®¡ã€ç»ˆæ­¢åˆ¤æ–­ç­‰é€»è¾‘
    âŒ éš¾ä»¥ä¸ç°æœ‰engineé›†æˆ
    
    æ–°è®¾è®¡ (loop):
    âœ… åªå¤„ç†å•æ­¥äº¤äº’
    âœ… ç®€åŒ–çš„æ•°æ®ç»“æ„ (prompt, response, action, reward)
    âœ… ä¸“æ³¨äºæœ€æ ¸å¿ƒçš„æ•°æ®æ›´æ–°
    âœ… æ˜“äºé›†æˆåˆ°ç°æœ‰engineä¸­
    
    èŒè´£åˆ†ç¦»:
    ğŸ“ loop()å‡½æ•°: å¤„ç†agentä¸æ¨¡å‹çš„äº¤äº’
    ğŸ“ engine: å¤„ç†å¾ªç¯æ§åˆ¶ã€è½¨è¿¹ç®¡ç†ã€æ€§èƒ½ç»Ÿè®¡
    ğŸ“ ç¯å¢ƒ: å¤„ç†ç¯å¢ƒäº¤äº’å’Œå¥–åŠ±è®¡ç®—
    """
    
    print(comparison)


def usage_in_engine_example():
    """å±•ç¤ºåœ¨engineä¸­çš„ä½¿ç”¨æ–¹å¼"""
    print("\nğŸ”§ åœ¨Engineä¸­çš„ä½¿ç”¨ç¤ºä¾‹:")
    print("=" * 40)
    
    engine_usage = '''
    # åœ¨Agent Execution Engineä¸­çš„ä½¿ç”¨æ–¹å¼
    
    class MultiAgentExecutionEngine(AgentExecutionEngine):
        def __init__(self, agent_graph, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.agent_graph = agent_graph
        
        async def run_multi_agent_trajectory(self, env, max_steps=5):
            """è¿è¡Œå¤šagentè½¨è¿¹"""
            obs, _ = env.reset()
            
            trajectory_data = {
                "steps": [],
                "rewards": [],
                "total_reward": 0.0
            }
            
            for step_idx in range(max_steps):
                # è°ƒç”¨ç®€åŒ–çš„loopå‡½æ•°
                step_data = await self.agent_graph.loop(obs, step_idx)
                
                # å¤„ç†ç¯å¢ƒäº¤äº’
                step_rewards = []
                for agent_name, agent_data in step_data.items():
                    obs, reward, done, info = env.step(
                        agent_data["action_type"], 
                        agent_data["action"]
                    )
                    step_rewards.append(reward)
                
                # æ›´æ–°å¥–åŠ±
                env_results = {"default_reward": sum(step_rewards)}
                step_data = await self.agent_graph.update_rewards(
                    step_data, env_results
                )
                
                # å­˜å‚¨è½¨è¿¹æ•°æ®
                trajectory_data["steps"].append(step_data)
                trajectory_data["rewards"].extend(step_rewards)
                trajectory_data["total_reward"] += sum(step_rewards)
                
                if done:
                    break
            
            return trajectory_data
    '''
    
    print(engine_usage)


if __name__ == "__main__":
    print("ğŸ¯ ç®€åŒ–Multi-Agent Loopè®¾è®¡æ¼”ç¤º")
    print("ğŸ“˜ æ–°è®¾è®¡ä¸“æ³¨äºæœ€æ ¸å¿ƒçš„åŠŸèƒ½ï¼Œä¾¿äºä¸engineé›†æˆ")
    print()
    
    # å±•ç¤ºæ•°æ®ç»“æ„
    show_data_structure()
    
    # å¯¹æ¯”è®¾è®¡
    compare_old_vs_new()
    
    # Engineä½¿ç”¨ç¤ºä¾‹
    usage_in_engine_example()
    
    # è¿è¡Œæ¼”ç¤º
    print("\nğŸš€ è¿è¡Œå®é™…æ¼”ç¤º...")
    asyncio.run(demonstrate_simplified_loop()) 