defaults:
  # Inject a separate ppo_trainer/base subtree for each model; can be overridden independently
  - ppo_trainer@models.model_0.ppo_trainer_config: base
  - _self_

# dataset config (shared by all agents)
data:
  #rollout config
  gen_batch_size: 32
  gen_n_samples: 1
  sample_temperature: 0

  # Batch sizes
  train_batch_size: 16
  val_batch_size: 8
  
  # Sequence lengths
  max_prompt_length: 512
  max_response_length: 512

resource:
  nnodes: 1
  n_gpus_per_node: 4
  trust_remote_code: true

# Environment configuration (shared by all agents)
env:
  name: code_env
  benchmark: "CodeForces"
  max_turns: 4
  resolve: false
  multi_modal: false
  batched_init: true

# Multi-agent interaction configuration
multi_agent_interaction:
  
  # Turn order for agents (list of agent names)
  turn_order: ["code_generator", "test_generator"]
  
  # Number of agents that interact per episode
  num_interacting_agents: 2
  
  # Whether agents can see other agents' actions
  shared_observation: true

    # ppo_trainer_config is injected via defaults; the above settings override the defaults in base
    

# Multi-agent configuration for two policies
agent_policy_configs:
  # Number of agents to train
  num_agents: 2
  policy_list: ["code_generator", "test_generator"]
  agent_configs:
    agent_0:
      name: "code_generator"
      policy_name: "code_generator_model"
      sample_num: 4
      
    agent_1:
      name: "test_generator"
      policy_name: "code_generator_model"
      sample_num: 4

project_name: pettingllms
experiment_name: code_single_policy
logger: [ 'console', 'wandb' ]

trainer:
  ${models.model_0.ppo_trainer_config.trainer}
 