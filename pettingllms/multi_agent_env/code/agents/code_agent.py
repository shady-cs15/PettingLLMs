import copy
import logging
from typing import Any
from pettingllms.multi_agent_env.base.agent import Agent, AgentData
from pettingllms.multi_agent_env.base.env import Env
from pettingllms.utils.logger_config import get_multi_logger

logger = logging.getLogger(__name__)


def truncatefn(s, length=300):
    if isinstance(s, str):
        pass
    else:
        s = str(s)
    if len(s) <= length:
        return s

    return s[: length // 2] + "...(truncated) ..." + s[-length // 2 :]


class CodeGenerationAgent(Agent):
    """
    Agent specialized for generating code to solve programming problems.
    """

    def __init__(self, rollout_idx: int | None = None, **kwargs):
        """
        Initialize the Code Generation Agent's data.
        """
        super().__init__()
        self.rollout_idx = rollout_idx
        # Accept other unrelated keyword arguments for compatibility
        for key, value in (kwargs or {}).items():
            setattr(self, key, value)
        
        # 初始化多日志系统
        self.multi_logger = get_multi_logger()

    def update_from_env(self, env_data: Env):
        # Save environment data
        self.env_data = env_data

        # Support passing either the raw environment (with state) or a wrapped Env
        state = getattr(env_data, "state", None)
        agent_obs = getattr(env_data, "agent_observations", None)

        def as_text(value: Any) -> str:
            if value is None:
                return ""
            if isinstance(value, list):
                return "\n".join([str(v) for v in value])
            return str(value)
        

        question = getattr(state, "problem", None)
        current_code = getattr(state, "generated_code", None)
        current_test_input = getattr(state, "generated_test_input", None)
        current_test_output = getattr(state, "generated_test_output", None)
        current_code_output = getattr(state, "exe_code_generated_test_output", None)
        need_generate = current_code in (None, "") or current_test_input in (None, "") or current_code_output in (None, "")

        if need_generate:
            # Generation mode
            formatted_prompt = (
                f"You are a helpful assistant that generates code to solve programming problems.\n\n"
                f"You need to think first then write python script."
                f"You should use input() to input and print() to output in your script.\n```"
                f"Problem:\n{question}\n\n"
                f"Please generate correct, efficient, and readable code that solves the problem and can pass comprehensive tests.\n\n"
                f"Respond in the format:\n\n"
                f"```python\n# your code here\n```\n\n"
               
            )
        else:
            # Refinement mode
            formatted_prompt = (
                f"You are a helpful assistant that refines code to pass tests. You need to think first then refine and generate new python script to pass all tests.\n\n"
                f"You need to think first then write python script."
                f"Problem:\n{question}\n\n"
                f"Current code:\n{as_text(current_code)}\n\n"
                f"but the execution result is not aligned with the test case outputs generated by another test case generator.\n"
                f"Current generated test cases (inputs):\n{as_text(current_test_input)}\n\n"
                f"Current generated test cases (outputs):\n{as_text(current_test_output)}\n\n"
                f"Current code execution result:\n{as_text(current_code_output)}\n\n"
                f"Please first judge the mismatch between the current generated test cases and the current code execution result, if the mismatch is caused by the current code, please refine the code to pass all tests.\n"
                f"Refine the code to pass all tests.\n\n"
                f"Respond in the format:\n\n"
                f"```python\n# corrected code here\n```\n\n"
            )

        self.current_prompt = {"text": formatted_prompt, "image": None}
        
    
    def update_from_model(self, response: str):
        # Parse the response and update agent_data
        import re
        
        # Parse code
        code = ""
        
        # Try to match the code block in our prompt format
        matches = re.findall(r"```python(.*?)```", response, re.DOTALL)
        if matches:
            code = matches[-1].strip()
        else:
            code = "We can not extract the code in the output. "

            # Update the agent's current action (environment expects a raw code string)
        self.current_action = code

        return self.current_action

    def calculate_reward(self, env_data: Env, mode: str = "sum") -> float:
        """
        Compute reward based on environment state.
        Uses generated_test_vs_generated_code_match_ratio for reward calculation.
        """
        state = getattr(env_data, "state", None)
        generated_pass_ratio = 0.0

        if state is not None:
            # Generated tests vs generated code
            gen_vs_ground_truth = getattr(state, "ground_truth_test_vs_generated_code_match_ratio", None)

            if isinstance(gen_vs_ground_truth, (int, float)):
                generated_pass_ratio = float(gen_vs_ground_truth)

        # Record and return
        self.agent_reward = generated_pass_ratio
        if self.info is None:
            self.info = {}
        self.info.update({
            "generated_pass_ratio": generated_pass_ratio,
            "reward_mode": "generated",
        })
        
        return generated_pass_ratio

    
    
    
    
    
    def reset(self):
        """
        Reset the agent's internal state for a new episode.
        """
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
        self.current_reward = None
        self.current_info = None
        self.current_action = None
        self.current_prompt = None
        self.current_response = None

 