import copy
import logging
from typing import Any
from pettingllms.multi_agent_env.base.agent import Agent, AgentData
from pettingllms.multi_agent_env.base.env import Env
from pettingllms.utils.logger_config import get_multi_logger
from typing import List
from pettingllms.multi_agent_env.math.math_utils import  extract_code
logger = logging.getLogger(__name__)


def truncatefn(s, length=300):
    if isinstance(s, str):
        pass
    else:
        s = str(s)
    if len(s) <= length:
        return s

    return s[: length // 2] + "...(truncated) ..." + s[-length // 2 :]


class ToolAgent(Agent):
    """
    Agent specialized for solving mathematical problems.
    """

    def __init__(self, rollout_idx: int | None = None, **kwargs):
        """
        Initialize the Math Solving Agent's data.
        """
        super().__init__()
        self.rollout_idx = rollout_idx
        # Accept other unrelated keyword arguments for compatibility
        for key, value in (kwargs or {}).items():
            setattr(self, key, value)
        
        # 初始化多日志系统
        self.multi_logger = get_multi_logger()

    def update_from_env(self, env_data: Env):
        # Save environment data
        self.env_data = env_data

        # Support passing either the raw environment (with state) or a wrapped Env
        state = getattr(env_data, "state", None)
        agent_obs = getattr(env_data, "agent_observations", None)

        def as_text(value: Any) -> str:
            if value is None:
                return ""
            if isinstance(value, list):
                return "\n".join([str(v) for v in value])
            return str(value)
        

        problem = getattr(state, "problem", None)
        code_solution = getattr(state, "code_generated_solution", None)
        code_extracted_answer = getattr(state, "code_extracted_answer", None)
        reasoning_solution = getattr(state, "reasoning_generated_solution", None)
        reasoning_extracted_answer = getattr(state, "reasoning_extracted_answer", None)
        
        need_generate = code_solution in (None, "") or code_extracted_answer in (None, "")

        if need_generate:
            formatted_prompt = (
                f"You are a helpful assistant that solves mathematical problems through step-by-step reasoning.\n\n"
                f"You need to think step by step and provide a complete solution using python code with clear mathematical reasoning.\n"
                f"Please write Python code to solve this problem.\n And you need to print the final answer in the code. Like if the final anwer is the variable x, you need to write ```print(x)```.\n"
                f"Respond in the format:\n\n"
                f"**Code:**\n```python\n# corrected code here\n```\n\n" 
            )
        else:
            formatted_prompt = (
                f"You are a helpful assistant that refines mathematical solutions through reasoning.\n\n"
                f"Problem:\n{problem}\n\n"
                f"Your previous reasoning solution:\n{as_text(code_solution)}\n\n And your extracted answer is {code_extracted_answer}.\n"
                f"But your extracted answer is mismatch with the answer generated by another LLM directly solve the problem using reasoning.\n"
                f"The reasoning agent's solution is {reasoning_solution}\n"
                f"The reasoning agent's answer is {reasoning_extracted_answer}\n"
                f"Please firstly judge the mismatch is caused by the reasoning agent's solution or your code solution. And then solve the problem again.\n"
            )
            
            formatted_prompt += (
                f"Respond in the format:\n\n"
                f"**Code:**\n```python\n# corrected code here\n```\n\n" 
            )
        
        self.current_prompt = {"text": formatted_prompt, "image": None}
        
    
    def update_from_model(self, response: str):
        # Parse the response and update agent_data
        self.current_action = extract_code(response)
        return self.current_action

    async def step(self, env_data: Env, env_worker: Any = None):
        """
        Process the generated code solution and evaluate it against the ground truth.
        """
        generated_solution = self.current_action
        env_data.state.code_generated_solution = generated_solution

        # 2) Extract answer from the code solution
        extracted_answer = extract_answer(generated_solution)
        env_data.state.code_extracted_answer = extracted_answer

        # 3) Evaluate correctness
        ground_truth_answer = env_data.state.ground_truth_answer
        is_correct = False
        
        if extracted_answer is not None and ground_truth_answer is not None:
            try:
                is_correct = grade_answer_verl(generated_solution, ground_truth_answer)
                # 存储代码智能体的正确性（如果需要的话，可以添加code_is_correct字段）
                # 这里暂时使用通用的is_correct字段，但可以根据需要分离
                if not hasattr(env_data.state, 'code_is_correct'):
                    env_data.state.code_is_correct = is_correct
                else:
                    env_data.state.code_is_correct = is_correct
                
                if is_correct:
                    self.done = True
                    self.is_pass = True
                    
            except Exception as e:
                print(f"Warning: Failed to evaluate code solution: {e}")
                is_correct = False
                if not hasattr(env_data.state, 'code_is_correct'):
                    env_data.state.code_is_correct = False
                else:
                    env_data.state.code_is_correct = False
        else:
            if not hasattr(env_data.state, 'code_is_correct'):
                env_data.state.code_is_correct = False
            else:
                env_data.state.code_is_correct = False

        # 4) Update reward based on correctness
        if len(self.reward_history) > 0:
            self.agent_reward = float(is_correct) - self.reward_history[-1]
        else:
            self.agent_reward = float(is_correct)
        self.reward_history.append(float(is_correct))

    def calculate_reward(self, env_data: List[Env]) -> float:
        """
        Compute reward based on environment state.
        Uses correctness for reward calculation.
        """
        state = getattr(env_data[0], "state", None)
        correctness = 0.0

        if state is not None:
            is_correct = getattr(state, "is_correct", None)
            if isinstance(is_correct, bool):
                correctness = float(is_correct)

        # Record and return
        self.agent_reward = correctness
        self.reward_history.append(self.agent_reward)
        
        return self.agent_reward
    
    def reset(self):
        """
        Reset the agent's internal state for a new episode.
        """
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
        self.current_reward = None
        self.current_info = None
        self.current_action = None
        self.current_prompt = None
        self.current_response = None
