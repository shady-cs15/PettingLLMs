# scripts/dataprocess/load_train_code_livecodebench.py
# pip install -U datasets pandas pyarrow huggingface_hub

from datasets import load_dataset
import pandas as pd
from pathlib import Path
import os, json, ast
import numpy as np
from typing import List, Tuple, Union

# ---- mapping: å“ªäº›æ–‡ä»¶å±äºå“ªä¸ªç‰ˆæœ¬ ----
# ä»“åº“: https://huggingface.co/datasets/livecodebench/code_generation_lite (å« test*.jsonl)
HF_PREFIX = "hf://datasets/livecodebench/code_generation_lite/"
V1_V5_FILES = [f"{HF_PREFIX}test{i}.jsonl" for i in ["", "2", "3", "4", "5"]]  # test.jsonl..test5.jsonl
V6_FILES   = [f"{HF_PREFIX}test6.jsonl"]  # v6
def _parse_tests(raw) -> tuple[list[str], list[str]]:
    """
    å°† public/private_test_cases è§£æä¸º (inputs, outputs)
    å…¼å®¹å‡ ç§æƒ…å†µï¼š
    1) å­—å…¸: {"input":[...], "output":[...]} æˆ– {"tests":[{"input":..,"output":..}, ...]}
    2) é¡¶å±‚åˆ—è¡¨: [{"input":..,"output":..}, ...]
    3) åŒé‡ç¼–ç : å­—ç¬¦ä¸²é‡Œè¿˜æ˜¯ JSONï¼ˆå†è§£ä¸€å±‚ï¼‰
    4) å…œåº•: è§£æå¤±è´¥æ—¶ï¼Œè¿”å› ([åŸå§‹å­—ç¬¦ä¸²], [])
    """
    def _loads_with_fallbacks(x):
        """
        å°è¯•å¤šè½®ååºåˆ—åŒ–ï¼š
        - è¿ç»­ json.loadsï¼Œç›´åˆ°ä¸æ˜¯ str
        - è‹¥ json å¤±è´¥ï¼Œä½¿ç”¨ ast.literal_eval å…œåº•ï¼ˆå¤„ç†å•å¼•å·/ç±»ä¼¼ Python å­—é¢é‡ï¼‰
        """
        if not isinstance(x, str):
            return x

        s = x
        # æœ€å¤šå°è¯• 3 è½®â€œè§£å¼€æ´‹è‘±â€
        for _ in range(3):
            # å…ˆè¯• JSON
            try:
                y = json.loads(s)
                if isinstance(y, str):
                    s = y
                    continue
                return y
            except Exception:
                # å†è¯• Python å­—é¢é‡
                try:
                    y = ast.literal_eval(s)
                    if isinstance(y, str):
                        s = y
                        continue
                    return y
                except Exception:
                    break
        return x

    data = _loads_with_fallbacks(raw)

    def _normalize_input_value(v) -> str:
        # å°†å•ä¸ªâ€œæµ‹è¯•ç”¨ä¾‹çš„è¾“å…¥â€ç»Ÿä¸€ä¸ºå¤šè¡Œå­—ç¬¦ä¸²ï¼šé€’å½’æ‰å¹³åŒ–åé€å€¼ä¸€è¡Œï¼Œå¹¶è¿½åŠ æœ«å°¾æ¢è¡Œ
        if v is None:
            return ""
        flat: list[str] = []
        def _flatten(x):
            if isinstance(x, (list, tuple, np.ndarray)):
                for xi in list(x):
                    _flatten(xi)
            else:
                flat.append(str(x))
        _flatten(v)
        if not flat and isinstance(v, str):
            # ä¸æ˜¯å¯è¿­ä»£ç»“æ„ï¼Œç›´æ¥è¿”å›åŸå­—ç¬¦ä¸²ï¼ˆè‹¥æ— æ¢è¡Œåˆ™è¿½åŠ ï¼‰
            s = v
            return s if s.endswith("\n") else s + "\n"
        s = "\n".join(flat)
        return s if s.endswith("\n") else s + "\n"

    def _normalize_output_value(v) -> str:
        # å°†å•ä¸ªâ€œæœŸæœ›è¾“å‡ºâ€ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²ï¼›è‹¥å¤šè¡Œåˆ™æŒ‰æ¢è¡Œæ‹¼æ¥ï¼Œä½†ä¸å¼ºåˆ¶æœ«å°¾æ¢è¡Œ
        if v is None:
            return ""
        if isinstance(v, (list, tuple, np.ndarray)):
            lines: list[str] = []
            for item in list(v):
                if isinstance(item, (list, tuple, np.ndarray)):
                    lines.append(" ".join(str(x) for x in list(item)))
                else:
                    lines.append(str(item))
            return "\n".join(lines)
        return str(v)

    # æƒ…å†µ A: dict ç»“æ„
    if isinstance(data, dict):
        # A1: {"input":[...], "output":[...]}
        if "input" in data and "output" in data:
            ins = data.get("input") or []
            outs = data.get("output") or []
            ins = ins if isinstance(ins, list) else [ins]
            outs = outs if isinstance(outs, list) else [outs]
            norm_ins = [_normalize_input_value(x) for x in ins]
            norm_outs = [_normalize_output_value(x) for x in outs]
            return norm_ins, norm_outs

        # A2: {"tests":[{"input":...,"output":...}, ...]}
        if "tests" in data and isinstance(data["tests"], list):
            ins, outs = [], []
            for t in data["tests"]:
                if isinstance(t, dict):
                    # å…¼å®¹ä¸åŒé”®å
                    input_val = t.get("input", t.get("inputs", ""))
                    output_val = t.get("output", t.get("expected_output", t.get("outputs", "")))
                    ins.append(_normalize_input_value(input_val))
                    outs.append(_normalize_output_value(output_val))
                else:
                    # è‹¥å…ƒç´ ä¸æ˜¯ dictï¼Œå°±å½“æˆå­—ç¬¦ä¸²å…œåº•
                    ins.append(_normalize_input_value(t))
                    outs.append("")
            return ins, outs

    # æƒ…å†µ B: é¡¶å±‚å°±æ˜¯ list -> æ¯ä¸ªå…ƒç´ é€šå¸¸æ˜¯ {"input":...,"output":...}
    if isinstance(data, list):
        ins, outs = [], []
        for item in data:
            if isinstance(item, dict):
                input_val = item.get("input", item.get("inputs", ""))
                output_val = item.get("output", item.get("expected_output", item.get("outputs", "")))
                ins.append(_normalize_input_value(input_val))
                outs.append(_normalize_output_value(output_val))
            else:
                ins.append(_normalize_input_value(item))
                outs.append("")
        return ins, outs

    # æƒ…å†µ C: å…œåº•
    try:
        # æœ€åä¸€æ¬¡å°è¯•ï¼šå¦‚æœè¿˜æ˜¯å­—ç¬¦ä¸²ï¼Œçœ‹çœ‹æ˜¯å¦åƒ JSON/list ç»“æ„
        if isinstance(raw, str):
            s = raw.strip()
            if (s.startswith("[") and s.endswith("]")) or (s.startswith("{") and s.endswith("}")):
                maybe = _loads_with_fallbacks(s)
                if isinstance(maybe, (list, dict)):
                    return _parse_tests(maybe)
        return ([str(raw)], [])
    except Exception:
        return [], []

def _hf_jsonl_to_df(data_files: List[str]) -> pd.DataFrame:
    # åˆ©ç”¨é€šç”¨ JSON Loader ç›´æ¥è¯» Hub æ–‡ä»¶
    ds = load_dataset("json", data_files=data_files, split="train")  
    rows = []
    for ex in ds:
        title   = ex.get("question_title") or ""
        content = ex.get("question_content") or ""
        question = (title + ("\n\n" if title and content else "") + content).strip()

        pub_raw = ex.get("public_test_cases") or ""
        pri_raw = ex.get("private_test_cases") or ""
        pub_in, pub_out = _parse_tests(pub_raw)
        pri_in, pri_out = _parse_tests(pri_raw)

        test_input  = pub_in if pub_in else pri_in
        test_output = pub_out if pub_out else pri_out

        rows.append({
            "question": question,
            "solution": "",  # LCB æ— å‚è€ƒå®ç°ï¼Œè¿™é‡Œå ä½ä»¥å…¼å®¹ä¸‹æ¸¸
            "test_input": test_input,
            "test_output": test_output,
            "difficulty": ex.get("difficulty"),
            "name": ex.get("question_id") or "",
            # è¿™äº›å­—æ®µä½ è‹¥éœ€è¦å¯ä¿ç•™ï¼š
            # "platform": ex.get("platform") or "",
            # "contest_id": ex.get("contest_id") or "",
            # "contest_date": ex.get("contest_date") or "",
            # "starter_code": ex.get("starter_code") or "",
        })
    def _ensure_list_of_str(x):
        # å°† ndarray/tuple/None/æ ‡é‡ ç­‰ç»Ÿä¸€è½¬ä¸º list[str]
        if x is None:
            return []
        if isinstance(x, (np.ndarray, tuple, set)):
            x = list(x)
        if isinstance(x, (str, bytes)):
            return [str(x)]
        if isinstance(x, list):
            return [str(i) for i in x]
        # å…¶ä»–æ ‡é‡
        try:
            return [str(x)]
        except Exception:
            return []

    df = pd.DataFrame(rows)
    wanted = ["question", "solution", "test_input", "test_output", "difficulty", "name"]
    for c in wanted:
        if c not in df.columns:
            df[c] = []
    # è§„èŒƒåŒ–ä¸¤åˆ—ï¼šä¿è¯ä¸º Python list[str]
    df["test_input"] = df["test_input"].apply(_ensure_list_of_str)
    df["test_output"] = df["test_output"].apply(_ensure_list_of_str)
    return df[wanted]

def main():
    # è¾“å‡ºåˆ° datasets/code/livecodebench/{train.parquet,test.parquet}
    project_root = Path(__file__).resolve().parents[2]
    out_dir = project_root / "datasets" / "code" / "livecodebench"
    test_dir = project_root / "datasets" / "code" /"train" 
    os.makedirs(out_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {out_dir}")

    print("ğŸ”„ åŠ è½½ LiveCodeBench v1â€“v5 ä½œä¸º train ...")
    train_df = _hf_jsonl_to_df(V1_V5_FILES)
    print("ğŸ”„ åŠ è½½ LiveCodeBench v6 ä½œä¸º test ...")
    test_df  = _hf_jsonl_to_df(V6_FILES)

    train_pq = out_dir / "train.parquet"
    test_pq  = test_dir / "test.parquet"
    train_df.to_parquet(train_pq, index=False)
    test_df.to_parquet(test_pq, index=False)
    print(f"ğŸ’¾ å·²ä¿å­˜ train.parquet -> {train_pq}")
    print(f"ğŸ’¾ å·²ä¿å­˜ test.parquet  -> {test_pq}")

    # ç®€å•ç»Ÿè®¡
    try:
        td = pd.read_parquet(test_pq)
        print("=== Difficulty value counts (test) ===")
        print(td["difficulty"].value_counts().sort_index())
        def _seq_len(x):
            if isinstance(x, (list, tuple, np.ndarray)):
                try:
                    return len(x)
                except Exception:
                    return 0
            return 0
        tcnt = td["test_input"].map(_seq_len)
        print(f"\n=== test_input stats ===")
        print(f"Max #inputs: {int(tcnt.max()) if len(tcnt) else 0}")
        print(f"Min #inputs: {int(tcnt.min()) if len(tcnt) else 0}")
    except Exception:
        pass

if __name__ == "__main__":
    main()
