# pip install datasets pandas pyarrow
from datasets import load_dataset
import pandas as pd
from pathlib import Path
import os
import re, ast, json, zlib, pickle, base64
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass
from enum import Enum
from datetime import datetime

# ============================================================
# å…¬å…±ï¼šæŠŠä»»æ„â€œå‚æ•°/å­—é¢é‡è¯´æ˜â€è§„çº¦ä¸º stdin é£æ ¼çš„å¤šè¡Œæ–‡æœ¬
#ï¼ˆå‚è€ƒä½ æä¾›çš„ transform_* / replace_* æ€è·¯ï¼‰
# ============================================================

def _find_matching_bracket(s: str, start: int) -> Optional[int]:
    depth = 0
    for i in range(start, len(s)):
        if s[i] == '[':
            depth += 1
        elif s[i] == ']':
            depth -= 1
            if depth == 0:
                return i
    return None

def transform_tokens(s: str) -> str:
    """
    æŠŠè¾“å…¥ä¸² s è§„çº¦ä¸º stdin å¤šè¡Œï¼š
      â€¢ 2D æ•°ç»„ [[...], ...] â†’ æ¯è¡Œ â€œx y â€¦â€
      â€¢ 1D æ•°ç»„ [a,b,c]      â†’ â€œa b câ€
      â€¢ å¼•å·å­—ç¬¦ä¸²           â†’ å»å¼•å·
      â€¢ å…¶å®ƒ token           â†’ åŸæ ·
    è¾“å‡ºä»¥ '\n' ç»“å°¾ã€‚
    """
    events = []
    masked = s

    # å…ˆæŠ“ç¬¬ä¸€æ®µ [[...]]ï¼Œé¿å…å†…éƒ¨å†è¢«æ‹†
    start2 = s.find('[[')
    if start2 != -1:
        end2 = _find_matching_bracket(s, start2)
        if end2 is not None:
            arr_lit = s[start2:end2+1]
            try:
                arr2d = ast.literal_eval(arr_lit)
            except Exception:
                arr2d = []
            events.append((start2, 'array2d', arr2d))
            masked = masked[:start2] + ' '*(end2+1 - start2) + masked[end2+1:]

    token_re = re.compile(r'\[[^\]]*\]|"(?:\\.|[^"\\])*"|\'(?:\\.|[^\'\\])*\'|\S+')
    for m in token_re.finditer(masked):
        tok = m.group(0); pos = m.start()
        if tok.startswith('[') and tok.endswith(']'):
            try:
                raw = ast.literal_eval(tok)
                parts = [str(x) for x in raw]
            except Exception:
                parts = [x.strip() for x in tok.strip('[]').split(',') if x.strip()]
            events.append((pos, 'array1d', parts))
        elif (tok.startswith('"') and tok.endswith('"')) or (tok.startswith("'") and tok.endswith("'")):
            events.append((pos, 'scalar', tok[1:-1]))
        else:
            events.append((pos, 'scalar', tok))

    events.sort(key=lambda e: e[0])

    out = []
    for _, typ, data in events:
        if typ == 'scalar':
            out.append(str(data))
        elif typ == 'array1d':
            out.append(" ".join(data))
        else:
            for row in data:
                out.append(" ".join(map(str, row)))
    return "\n".join(out) + "\n"

def transform_input_block(spec: str) -> str:
    """
    â€œkey = value / è£¸ valueâ€çš„è‡ªç„¶è¯­è¨€å— â†’ stdin å¤šè¡Œã€‚
    """
    events: List[Tuple[int, str, object]] = []
    token_re = re.compile(
        r"""
        (?P<kv_array>      \b\w+\s*=\s*\[[^\[\]]*(?:\[[^\[\]]*\][^\[\]]*)*\])
      | (?P<kv_scalar>     \b\w+\s*=\s*(?: "(?:\\.|[^"\\])*" | '(?:\\.|[^'\\])*' | True | False | -?\d+(?:\.\d+)? | \w+))
      | (?P<array>         \[[^\[\]]*(?:\[[^\[\]]*\][^\[\]]*)*\])
      | (?P<scalar>        "(?:\\.|[^"\\])*" | '(?:\\.|[^'\\])*' | True | False | -?\d+(?:\.\d+)? | \w+)
        """,
        re.X,
    )

    for m in token_re.finditer(spec):
        if m.group("kv_array"):
            lit = m.group("kv_array").split("=", 1)[1].lstrip()
            try: arr = ast.literal_eval(lit)
            except Exception: arr = []
            events.append((m.start(), "array", arr)); continue
        if m.group("kv_scalar"):
            val = m.group("kv_scalar").split("=", 1)[1].lstrip()
            if val and val[0] in "\"'":
                val = val[1:-1]
            events.append((m.start(), "scalar", val)); continue
        if m.group("array"):
            try: arr = ast.literal_eval(m.group("array"))
            except Exception: arr = []
            events.append((m.start(), "array", arr)); continue
        if m.group("scalar"):
            tok = m.group("scalar")
            if tok and tok[0] in "\"'":
                tok = tok[1:-1]
            events.append((m.start(), "scalar", tok))

    events.sort(key=lambda e: e[0])
    lines: List[str] = []
    for _, kind, val in events:
        if kind == "scalar":
            lines.append(str(val))
        else:
            if isinstance(val, list) and val and all(isinstance(r, list) for r in val):
                lines.extend(" ".join(map(str, r)) for r in val)
            else:
                lines.append(" ".join(map(str, val)))
    return "\n".join(lines) + "\n"

def replace_input_block(text: str) -> str:
    def _repl(m):
        return f"{m.group(1)}\n{transform_input_block(m.group(2))}"
    pattern = re.compile(r'(Input\s*:\s*)(.*?)(?=\s*(?:Output\s*:|$))', flags=re.I|re.S)
    return pattern.sub(_repl, text)

def replace_output_block(text: str) -> str:
    def strip_quotes(tok: str) -> str:
        return tok[1:-1] if len(tok) >= 2 and tok[0] in "\"'" and tok[-1] == tok[0] else tok
    out, last = [], 0
    for m in re.finditer(r'Output\s*:', text):
        out.append(text[last:m.end()]); i = m.end()
        while i < len(text) and text[i].isspace():
            out.append(text[i]); i += 1
        if i >= len(text): break
        if text[i] == '[':
            start, end = i, _find_matching_bracket(text, i)
            literal = text[start:end+1] if end is not None else "[]"
            try: arr = ast.literal_eval(literal)
            except Exception: arr = []
            lines = ([" ".join(map(str, r)) for r in arr]
                     if arr and all(isinstance(r, list) for r in arr)
                     else [" ".join(map(str, arr))])
            out.append("\n" + "\n".join(lines) + "\n")
            last = (end or i) + 1
        else:
            m2 = re.match(r'(-?\d+|True|False|"(?:\\.|[^"\\])*"|\'(?:\\.|[^\'\\])*\')', text[i:])
            if m2:
                out.append(f"\n{strip_quotes(m2.group(0))}\n")
                last = i + len(m2.group(0))
            else:
                last = i
    out.append(text[last:])
    return "".join(out)

def _normalize_cell(s: str) -> str:
    s = (s or "").strip().replace("\\n", "\n")
    if not s.endswith("\n"):
        s += "\n"
    return s

# ============================================================
# æ–­è¨€ â†’ (input, output) æŠ½å–ï¼ˆMBPP / HumanEval ç”¨ï¼‰
# ============================================================

_ASSERT_PATTERNS: List[re.Pattern] = [
    # assert f(a,b,...) == expected
    re.compile(r'^\s*assert\s+(?P<call>\w+\s*\(.*\))\s*==\s*(?P<exp>.+?)\s*$', re.S),
    # assert expected == f(a,b,...)
    re.compile(r'^\s*assert\s+(?P<exp>.+?)\s*==\s*(?P<call>\w+\s*\(.*\))\s*$', re.S),
    # check(f(a,b,...), expected) / check_equal( ... )
    re.compile(r'^\s*(?:assert\s+)?(?:check|check_equal|check_solution)\s*\(\s*(?P<call>\w+\s*\(.*\))\s*,\s*(?P<exp>.+?)\s*\)\s*$', re.S),
    # check(expected, f(a,b,...))
    re.compile(r'^\s*(?:assert\s+)?(?:check|check_equal|check_solution)\s*\(\s*(?P<exp>.+?)\s*,\s*(?P<call>\w+\s*\(.*\))\s*\)\s*$', re.S),
]

def _strip_trailing_comment(s: str) -> str:
    return re.split(r'#(?![^\'"]*["\'])', s, maxsplit=1)[0].strip()

def _extract_args_from_call(call: str, prefer_fn: Optional[str] = None) -> Optional[str]:
    """
    ç»™å®š 'foo(1, [2,3], "x")' â†’ è¿”å›æ‹¬å·å†…çš„åŸå§‹å‚æ•°ä¸²ã€‚
    è‹¥ prefer_fn æä¾›ä¸” call ä¸æ˜¯è¯¥å‡½æ•°ï¼Œä»ç„¶æ¥å—ï¼ˆåŒ¹é…ä¸åˆ°å†é€€åŒ–ä¸ºä»»ä½•å‡½æ•°ï¼‰ã€‚
    """
    m = re.match(r'(?P<fn>\w+)\s*\((?P<args>.*)\)\s*$', call.strip(), re.S)
    if not m:
        return None
    fn = m.group("fn")
    if prefer_fn and fn != prefer_fn:
        # å…è®¸å…¶å®ƒå‡½æ•°ï¼ˆæœ‰äº›æµ‹è¯•åŒ…è£…å™¨å†…éƒ¨ä»ä¼šä¼ è¢«æµ‹å‡½æ•°ï¼‰
        pass
    return m.group("args")

def parse_asserts_to_io(lines: List[str], prefer_fn: Optional[str] = None) -> Tuple[List[str], List[str]]:
    """
    ä»æ–­è¨€è¡Œé‡ŒæŠ½å– (stdin åŒ–çš„ input, stdin åŒ–çš„ output) åˆ—è¡¨ã€‚
    """
    ins, outs = [], []
    for raw in lines:
        line = _strip_trailing_comment(raw)
        if not line:
            continue
        matched = None
        for pat in _ASSERT_PATTERNS:
            m = pat.match(line)
            if m:
                matched = m; break
        if not matched:
            continue
        call = matched.group("call")
        exp  = matched.group("exp")
        args = _extract_args_from_call(call, prefer_fn=prefer_fn)
        if args is None:
            continue
        # è§„çº¦ï¼šæŠŠå‚æ•°ä¸² / æœŸæœ›å€¼ä¸²è½¬ä¸º stdin é£æ ¼
        in_text  = _normalize_cell(transform_tokens(args))
        out_text = _normalize_cell(transform_tokens(exp))
        ins.append(in_text); outs.append(out_text)
    return ins, outs

# ============================================================
# CodeContests
# ============================================================

PY_LANG_IDS = {1, 3}  # deepmind/code_contests: 1=PYTHON(2), 3=PYTHON3

def _pick_first_py(solutions: Dict[str, Any], k: int = 1) -> List[str]:
    langs = solutions.get("language") or []
    codes = solutions.get("solution") or []
    out = []
    for lang, code in zip(langs, codes):
        if lang in PY_LANG_IDS:
            out.append(code)
            if len(out) >= k:
                break
    return out

def _has_py(solutions: Dict[str, Any]) -> bool:
    return any((l in PY_LANG_IDS) for l in (solutions.get("language") or []))

def process_code_contests(split: str) -> pd.DataFrame:
    print(f"ğŸ”„ åŠ è½½ deepmind/code_contests split={split} ...")
    ds = load_dataset("deepmind/code_contests", split=split)
    rows = []
    for ex in ds:
        tests = (ex.get("public_tests") or {}) if split == "test" else (ex.get("private_tests") or {})
        test_in = tests.get("input") or []
        test_out = tests.get("output") or []
        if not (isinstance(test_in, list) and test_in):
            continue
        solutions = ex.get("solutions") or {}
        if not _has_py(solutions):
            # å³ä¾¿æ²¡æœ‰ Python å‚è€ƒè§£ï¼Œä¹Ÿå…è®¸ï¼›solution ç½®ç©º
            solution = ""
        else:
            py = _pick_first_py(solutions, k=1)
            solution = py[0] if py else ""
        rows.append({
            "question": (ex.get("description") or "").strip(),
            "solution": solution,
            "test_input": [ _normalize_cell(str(x)) for x in test_in ],
            "test_output": [ _normalize_cell(str(x)) for x in (test_out or []) ],
        })
    df = pd.DataFrame(rows, columns=["question","test_input","test_output","solution"])
    print(f"âœ… code_contests/{split}: {len(df)}")
    return df

# ============================================================
# MBPP
# ============================================================

def process_mbpp() -> pd.DataFrame:
    print("ğŸ”„ åŠ è½½ MBPPï¼ˆä¼˜å…ˆ sanitized/testï¼‰...")
    try:
        ds = load_dataset("mbpp", name="sanitized", split="test")
    except Exception:
        ds = load_dataset("mbpp", split="test")
    rows = []
    for ex in ds:
        question = (ex.get("text") or ex.get("prompt") or ex.get("description") or "").strip()
        solution = (ex.get("code") or ex.get("solution") or "")

        # tests: list[str] æˆ–å•ä¸²
        test_list = ex.get("test_list") or ex.get("test") or []
        if isinstance(test_list, str):
            test_lines = [ln for ln in test_list.splitlines() if ln.strip()]
        elif isinstance(test_list, list):
            # æ™®éä¸ºæ–­è¨€å­—ç¬¦ä¸²åˆ—è¡¨
            test_lines = []
            for t in test_list:
                test_lines += [ln for ln in str(t).splitlines() if ln.strip()]
        else:
            test_lines = []

        # è§£ææ–­è¨€ â†’ I/O
        inputs, outputs = parse_asserts_to_io(test_lines, prefer_fn=None)
        rows.append({
            "question": question,
            "solution": solution or "",
            "test_input": inputs,
            "test_output": outputs,
        })
    df = pd.DataFrame(rows, columns=["question","test_input","test_output","solution"])
    print(f"âœ… mbpp: {len(df)}")
    return df

# ============================================================
# HumanEval
# ============================================================

def process_humaneval() -> pd.DataFrame:
    print("ğŸ”„ åŠ è½½ openai_humaneval/test ...")
    ds = load_dataset("openai_humaneval", split="test")
    rows = []
    for ex in ds:
        question = (ex.get("prompt") or "").strip()
        solution = (ex.get("canonical_solution") or ex.get("solution") or "")
        entry_point = ex.get("entry_point") or None
        test_str = ex.get("test") or ""
        test_lines = [ln for ln in str(test_str).splitlines() if ln.strip()]
        inputs, outputs = parse_asserts_to_io(test_lines, prefer_fn=entry_point)
        rows.append({
            "question": question,
            "solution": solution or "",
            "test_input": inputs,
            "test_output": outputs,
        })
    df = pd.DataFrame(rows, columns=["question","test_input","test_output","solution"])
    print(f"âœ… human_eval: {len(df)}")
    return df

# ============================================================
# LiveCodeBenchï¼ˆä½¿ç”¨ä½ æä¾›çš„ code_generation_lite + è§„çº¦ï¼‰
# ============================================================

class Platform(Enum):
    LEETCODE = "leetcode"
    CODEFORCES = "codeforces"
    ATCODER = "atcoder"

class Difficulty(Enum):
    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"

class TestType(Enum):
    STDIN = "stdin"
    FUNCTIONAL = "functional"

@dataclass
class _LCB_Test:
    input: str
    output: str
    testtype: TestType
    def __post_init__(self):
        self.testtype = TestType(self.testtype)

@dataclass
class _LCB_Problem:
    question_title: str
    question_content: str
    platform: Platform
    question_id: str
    contest_id: str
    contest_date: datetime
    starter_code: str
    difficulty: Difficulty
    public_test_cases: list[_LCB_Test]
    private_test_cases: list[_LCB_Test]
    metadata: dict
    def __post_init__(self):
        self.platform = Platform(self.platform)
        self.difficulty = Difficulty(self.difficulty)
        self.contest_date = datetime.fromisoformat(self.contest_date)
        # public
        pts = json.loads(self.public_test_cases)
        self.public_test_cases = [_LCB_Test(**t) for t in pts]
        # private å¯èƒ½æ˜¯ json æˆ– zlib+pickle+base64
        try:
            pr = json.loads(self.private_test_cases)
        except Exception:
            pr = json.loads(
                pickle.loads(
                    zlib.decompress(
                        base64.b64decode(self.private_test_cases.encode("utf-8"))
                    )
                )
            )
        self.private_test_cases = [_LCB_Test(**t) for t in pr]
        self.metadata = json.loads(self.metadata)

def _load_lcb_lite(release_version: str = "release_v2",
                   start_date: Optional[str] = None,
                   end_date: Optional[str] = None) -> List[_LCB_Problem]:
    raw = load_dataset(
        "livecodebench/code_generation_lite",
        split="test",
        version_tag=release_version,
    )
    problems = [_LCB_Problem(**p) for p in raw]
    if start_date:
        p0 = datetime.strptime(start_date, "%Y-%m-%d")
        problems = [p for p in problems if p.contest_date >= p0]
    if end_date:
        p1 = datetime.strptime(end_date, "%Y-%m-%d")
        problems = [p for p in problems if p.contest_date <= p1]
    print(f"LCB loaded: {len(problems)}")
    return problems

def process_livecodebench(release_version: str = "release_v2",
                          start_date: Optional[str] = None,
                          end_date: Optional[str] = None) -> pd.DataFrame:
    print(f"ğŸ”„ åŠ è½½ LiveCodeBench/code_generation_lite ({release_version}) ...")
    probs = _load_lcb_lite(release_version, start_date, end_date)
    rows = []
    for p in probs:
        # è§„èŒƒåŒ–é¢˜é¢ä¸­çš„ Input/Output å™è¿°ï¼ˆå¦‚æœæœ‰ï¼‰
        qtext = p.question_content.strip()
        qtext = replace_input_block(qtext)
        qtext = replace_output_block(qtext)

        if p.private_test_cases and p.private_test_cases[0].testtype.value == "functional":
            ins  = [_normalize_cell(transform_tokens(t.input))  for t in p.private_test_cases]
            outs = [_normalize_cell(transform_tokens(t.output)) for t in p.private_test_cases]
        else:
            ins  = [_normalize_cell(t.input)  for t in p.private_test_cases]
            outs = [_normalize_cell(t.output) for t in p.private_test_cases]

        rows.append({
            "question": qtext,
            "solution": "",  # LCB å®˜æ–¹é€šå¸¸ä¸æä¾›å®Œæ•´å‚è€ƒè§£ï¼Œè¿™é‡Œç•™ç©º
            "test_input": ins,
            "test_output": outs,
        })
    df = pd.DataFrame(rows, columns=["question","test_input","test_output","solution"])
    print(f"âœ… livecodebench: {len(df)}")
    return df

# ============================================================
# ä¸»æµç¨‹ï¼šå†™å‡º 4 ä»½ parquetï¼ˆä»… 4 åˆ—ï¼‰
# ============================================================

def main():
    # è¾“å‡ºç›®å½•ï¼šdatasets/code/train/
    project_root = Path(__file__).resolve().parents[2]
    out_dir = project_root / "datasets" / "code" / "train"
    os.makedirs(out_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {out_dir}")

    # 1) è®­ç»ƒé›†ï¼šCodeContests(train) â†’ train.parquet
    df_train = process_code_contests(split="train")
    (out_dir / "train.parquet").unlink(missing_ok=True)
    df_train.to_parquet(out_dir / "train.parquet", index=False)
    print(f"ğŸ’¾ ä¿å­˜: {out_dir / 'train.parquet'}")

    # 2) æµ‹è¯•é›†å››ä»½ï¼šå„è‡ªåå­—.parquetï¼ˆä»…å« 4 åˆ—ï¼‰
    # 2.1 CodeContests(test)
    df_cc_test = process_code_contests(split="test")
    (out_dir / "code_contests.parquet").unlink(missing_ok=True)
    df_cc_test.to_parquet(out_dir / "code_contests.parquet", index=False)
    print(f"ğŸ’¾ ä¿å­˜: {out_dir / 'code_contests.parquet'}")

    # 2.2 MBPP
    df_mbpp = process_mbpp()
    (out_dir / "mbpp.parquet").unlink(missing_ok=True)
    df_mbpp.to_parquet(out_dir / "mbpp.parquet", index=False)
    print(f"ğŸ’¾ ä¿å­˜: {out_dir / 'mbpp.parquet'}")

    # 2.3 HumanEval
    df_he = process_humaneval()
    (out_dir / "human_eval.parquet").unlink(missing_ok=True)
    df_he.to_parquet(out_dir / "human_eval.parquet", index=False)
    print(f"ğŸ’¾ ä¿å­˜: {out_dir / 'human_eval.parquet'}")

    # 2.4 LiveCodeBenchï¼ˆä½¿ç”¨ liteï¼‰
    df_lcb = process_livecodebench(release_version="release_v2")
    (out_dir / "livecodebench.parquet").unlink(missing_ok=True)
    df_lcb.to_parquet(out_dir / "livecodebench.parquet", index=False)
    print(f"ğŸ’¾ ä¿å­˜: {out_dir / 'livecodebench.parquet'}")

if __name__ == "__main__":
    main()
